#!/usr/bin/env python3
"""
US-Monitor UI ä»ªè¡¨æ¿
ä½¿ç”¨ Streamlit æ„å»ºä¸­æ–‡ç•Œé¢
"""

import os
import sys
import json
import html
import logging
from datetime import datetime, timedelta
from typing import Any, Dict, List, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import streamlit as st
import pandas as pd
from supabase import create_client

from scripts.entity_classification import (
    ENTITY_TYPE_FILTER_OPTIONS,
    merge_entity_metadata,
    normalize_entity_mentions,
)

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not logger.handlers:
    formatter = logging.Formatter(
        "%(asctime)s [%(levelname)s] %(name)s: %(message)s", datefmt="%Y-%m-%d %H:%M:%S"
    )
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="US-Monitor çƒ­ç‚¹åˆ†æ",
    page_icon="ğŸ‡ºğŸ‡¸",
    layout="wide",
    initial_sidebar_state="expanded",
)

# ä¸»é¢˜é…ç½® - æŠ€æœ¯é£æ ¼ï¼Œé«˜å¯¹æ¯”åº¦
THEME = {
    "bg_main": "#0b1020",
    "bg_panel": "#0f172a",
    "bg_card": "#111c34",
    "bg_sidebar": "#0a1328",
    "text_main": "#e6edf7",
    "text_body": "#c7d2e7",
    "text_muted": "#8ea0bf",
    "primary": "#8bd3ff",
    "accent": "#29f0ff",
    "border": "#1f2d49",
    "danger": "#ff6b7a",
    "warn": "#ffd166",
    "ok": "#67f7c2",
}


# ç”Ÿæˆ CSS
def get_css():
    t = THEME
    return f"""
<style>
    :root {{
        color-scheme: dark !important;
        --bg-main: {t["bg_main"]};
        --bg-panel: {t["bg_panel"]};
        --bg-card: {t["bg_card"]};
        --bg-sidebar: {t["bg_sidebar"]};
        --text-main: {t["text_main"]};
        --text-body: {t["text_body"]};
        --text-muted: {t["text_muted"]};
        --accent: {t["accent"]};
        --primary: {t["primary"]};
        --border: {t["border"]};
    }}

    html, body {{
        background: var(--bg-main) !important;
        color: var(--text-main) !important;
        font-family: "Inter", "SF Pro Text", "Segoe UI", sans-serif;
    }}

    /* é¡¶éƒ¨ç™½æ¡å’Œéƒ¨ç½²åŒºåŸŸï¼šç»Ÿä¸€æ·±è‰²ï¼Œé¿å…ä¸ä¸»ç•Œé¢å†²çª */
    [data-testid="stHeader"] {{
        background: var(--bg-panel) !important;
        border-bottom: 1px solid var(--border);
    }}
    [data-testid="stToolbar"] {{
        background: transparent !important;
    }}
    [data-testid="stAppDeployButton"] {{
        display: none !important;
    }}

    /* Streamlit å…¨å±€å®¹å™¨ */
    .stApp {{
        background: var(--bg-main) !important;
        color: var(--text-main) !important;
    }}
    [data-testid="stAppViewContainer"] {{
        background: linear-gradient(180deg, #0b1020 0%, #0a1120 100%) !important;
    }}
    [data-testid="stMain"] {{
        background: transparent !important;
    }}
    [data-testid="block-container"] {{
        max-width: 1500px;
        padding-top: 2rem;
        padding-bottom: 2rem;
    }}
    
    /* ä¾§è¾¹æ  */
    [data-testid="stSidebar"] {{
        background: var(--bg-sidebar) !important;
        border-right: 1px solid var(--border);
    }}
    
    /* ç»Ÿä¸€æ–‡æœ¬é¢œè‰²ï¼ˆæ— è§†ç³»ç»Ÿæ˜æš—æ¨¡å¼ï¼‰ */
    .stMarkdown, .stText, p, li, h1, h2, h3, h4, h5, h6, label, span, div {{
        color: var(--text-main) !important;
    }}
    
    /* ä¸»æ ‡é¢˜ */
    .main-header {{
        font-size: 2.1rem;
        font-weight: 700;
        letter-spacing: 0.2px;
        color: var(--text-main);
        margin-bottom: 1.2rem;
        border-bottom: 2px solid var(--accent);
        padding-bottom: 0.7rem;
        text-shadow: 0 0 20px rgba(41, 240, 255, 0.15);
    }}
    
    /* æŒ‡æ ‡å¡ç‰‡ */
    [data-testid="stMetric"] {{
        background: var(--bg-card);
        padding: 1rem;
        border-radius: 10px;
        border: 1px solid var(--border);
        box-shadow: inset 0 0 0 1px rgba(139, 211, 255, 0.05);
    }}
    
    [data-testid="stMetricLabel"] {{
        color: var(--text-muted) !important;
        font-weight: 600;
    }}
    
    [data-testid="stMetricValue"] {{
        color: var(--text-main) !important;
        font-family: "JetBrains Mono", "SF Mono", "Consolas", monospace;
        font-weight: 700;
    }}
    
    /* çƒ­ç‚¹å¡ç‰‡ */
    .hotspot-card {{
        background: var(--bg-card);
        padding: 1.25rem;
        border-radius: 10px;
        border: 1px solid var(--border);
        margin-bottom: 1rem;
        border-left: 3px solid var(--accent);
    }}
    
    .hotspot-card h4 {{
        color: var(--text-main);
        font-weight: 600;
        font-size: 1.1rem;
        margin-bottom: 0.75rem;
    }}
    
    .hotspot-card h5 {{
        color: var(--text-body);
        font-weight: 600;
    }}
    
    .hotspot-card p {{
        color: var(--text-body);
        font-size: 0.95rem;
        line-height: 1.55;
    }}
    
    .hotspot-card .meta-text {{
        color: var(--text-muted);
        font-size: 0.85rem;
    }}
    
    /* ä¿¡å·å¾½ç«  */
    .signal-high {{
        color: {t["danger"]};
        font-weight: 700;
    }}
    .signal-medium {{
        color: {t["warn"]};
        font-weight: 700;
    }}
    .signal-low {{
        color: {t["ok"]};
        font-weight: 700;
    }}
    
    /* åˆ†å‰²çº¿ */
    hr {{
        border: none;
        height: 1px;
        background: var(--border);
        margin: 1.5rem 0;
    }}
    
    /* æ»šåŠ¨æ¡ */
    ::-webkit-scrollbar {{
        width: 8px;
    }}
    ::-webkit-scrollbar-track {{
        background: var(--bg-main);
    }}
    ::-webkit-scrollbar-thumb {{
        background: #2b3e62;
        border-radius: 8px;
    }}
    ::-webkit-scrollbar-thumb:hover {{
        background: #3c5689;
    }}
    
    /* æŒ‰é’® */
    .stButton > button {{
        background: var(--bg-panel);
        color: var(--text-main);
        border: 1px solid var(--border);
        border-radius: 8px;
        font-weight: 600;
    }}
    .stButton > button:hover {{
        border-color: var(--primary);
        color: var(--primary) !important;
        box-shadow: 0 0 0 1px rgba(139, 211, 255, 0.35);
    }}
    
    /* è¾“å…¥ç»„ä»¶ */
    [data-baseweb="select"] > div,
    .stSelectbox > div > div,
    .stTextInput > div > div > input,
    .stNumberInput input {{
        background: var(--bg-panel) !important;
        color: var(--text-main) !important;
        border: 1px solid var(--border) !important;
        border-radius: 8px !important;
    }}
    [data-baseweb="select"] svg {{
        fill: var(--text-muted) !important;
    }}
    /* é€‰ä¸­å€¼å’Œå ä½ç¬¦åœ¨å‘½ä»¤æ¨¡å¼ä¸‹ä¿æŒå¯è¯» */
    [data-baseweb="select"] [data-testid="stMarkdownContainer"] p,
    [data-baseweb="select"] span,
    [data-baseweb="select"] input {{
        color: var(--text-main) !important;
        opacity: 1 !important;
    }}
    /* ä¸‹æ‹‰é€‰é¡¹èœå•ï¼ˆportal å¼¹å±‚ï¼‰ */
    [data-baseweb="popover"] {{
        background: transparent !important;
    }}
    [data-baseweb="popover"] [role="listbox"] {{
        background: #0f172a !important;
        border: 1px solid #8bd3ff !important;
        border-radius: 10px !important;
        box-shadow: 0 12px 32px rgba(0, 0, 0, 0.55) !important;
    }}
    [data-baseweb="popover"] [role="option"] {{
        background: #0f172a !important;
        color: #f8fafc !important;
        opacity: 1 !important;
        font-weight: 600 !important;
    }}
    [data-baseweb="popover"] [role="option"]:hover {{
        background: #1e3a5f !important;
        color: #ffffff !important;
    }}
    [data-baseweb="popover"] [aria-selected="true"][role="option"] {{
        background: #2d5b8f !important;
        color: #ffffff !important;
    }}
    /* å…¼å®¹ä¸åŒç‰ˆæœ¬ BaseWeb èœå•èŠ‚ç‚¹ */
    [role="listbox"],
    ul[role="listbox"] {{
        background: #0f172a !important;
        color: #f8fafc !important;
        border: 1px solid #8bd3ff !important;
    }}
    [role="option"] {{
        color: #f8fafc !important;
        background: #0f172a !important;
    }}
    [role="option"][aria-disabled="true"] {{
        color: #cbd5e1 !important;
        opacity: 0.95 !important;
    }}
    
    /* å•é€‰æŒ‰é’® */
    .stRadio > div {{
        color: var(--text-body);
    }}
    [data-testid="stRadio"] label {{
        background: transparent !important;
    }}
    
    /* é“¾æ¥æŒ‰é’® */
    .stLinkButton > button {{
        background: transparent;
        border: 1px solid var(--accent);
        color: var(--accent) !important;
        border-radius: 8px;
        font-weight: 600;
    }}
    .stLinkButton > button:hover {{
        background: rgba(41, 240, 255, 0.12);
        color: var(--text-main) !important;
    }}
    
    /* å±•å¼€æ¡† */
    [data-testid="stExpander"] {{
        background: var(--bg-card);
        border: 1px solid var(--border);
        border-radius: 10px;
    }}
    .streamlit-expanderHeader, [data-testid="stExpander"] summary {{
        color: var(--text-body) !important;
        background: var(--bg-card) !important;
    }}
    [data-testid="stExpander"] summary {{
        align-items: flex-start !important;
    }}
    [data-testid="stExpander"] summary p {{
        white-space: normal !important;
        overflow: visible !important;
        text-overflow: clip !important;
        word-break: break-word !important;
        line-height: 1.45 !important;
    }}
    
    /* Tabs / DataFrame / å›¾è¡¨ */
    [data-baseweb="tab-list"] {{
        gap: 4px;
    }}
    [data-baseweb="tab"] {{
        background: var(--bg-panel) !important;
        border: 1px solid var(--border) !important;
        border-radius: 8px 8px 0 0;
        color: var(--text-muted) !important;
    }}
    [aria-selected="true"][data-baseweb="tab"] {{
        color: var(--text-main) !important;
        border-color: var(--primary) !important;
    }}
    .stDataFrame, [data-testid="stChart"] {{
        background: var(--bg-card);
        border: 1px solid var(--border);
        border-radius: 10px;
    }}
    /* éšè—å…ƒç´ å·¥å…·æ ï¼ˆå‘½ä»¤æ¨¡å¼ä¸‹å¯¹æ¯”åº¦ä¸ç¨³å®šï¼Œå½±å“å¯è¯»æ€§ï¼‰ */
    [data-testid="stElementToolbar"] {{
        display: none !important;
    }}
    /* Vega/Altair tooltip å¯¹æ¯”åº¦ä¿®å¤ */
    .vg-tooltip,
    .vega-embed .vg-tooltip {{
        background: var(--bg-panel) !important;
        color: var(--text-main) !important;
        border: 1px solid var(--border) !important;
        border-radius: 8px !important;
        box-shadow: 0 8px 24px rgba(0, 0, 0, 0.35) !important;
        font-size: 14px !important;
        line-height: 1.5 !important;
    }}
    .vg-tooltip td,
    .vg-tooltip th,
    .vega-embed .vg-tooltip td,
    .vega-embed .vg-tooltip th {{
        color: var(--text-main) !important;
    }}
    
    /* åˆ†ç±»æ ‡ç­¾ */
    .category-military {{ color: {t["danger"]}; font-weight: 600; }}
    .category-politics {{ color: #b39cff; font-weight: 600; }}
    .category-economy {{ color: {t["ok"]}; font-weight: 600; }}
    .category-tech {{ color: #67e8f9; font-weight: 600; }}
    
</style>
"""


# åº”ç”¨ä¸»é¢˜ CSS
st.markdown(get_css(), unsafe_allow_html=True)


# åˆå§‹åŒ– Supabase
@st.cache_resource
def init_supabase():
    """åˆå§‹åŒ– Supabase å®¢æˆ·ç«¯"""
    url = os.getenv("SUPABASE_URL")
    key = os.getenv("SUPABASE_KEY")

    if not url or not key:
        st.error("ç¼ºå°‘ Supabase é…ç½®ã€‚è¯·è®¾ç½® SUPABASE_URL å’Œ SUPABASE_KEY ç¯å¢ƒå˜é‡ã€‚")
        return None

    return create_client(url, key)


# æ•°æ®è·å–å‡½æ•°
@st.cache_data(ttl=300)  # ç¼“å­˜5åˆ†é’Ÿ
def get_clusters(
    _supabase, hours: int = 24, category: str = None, only_hot: bool = False
) -> pd.DataFrame:
    """è·å–èšç±»æ•°æ®"""
    cutoff = (datetime.now() - timedelta(hours=hours)).isoformat()

    query = _supabase.table("analysis_clusters").select("*").gte("created_at", cutoff)

    if category and category != "å…¨éƒ¨":
        query = query.eq("category", category)
    if only_hot:
        query = query.eq("is_hot", True)

    result = query.order("created_at", desc=True).execute()

    if result.data:
        return pd.DataFrame(result.data)
    return pd.DataFrame()


@st.cache_data(ttl=300)
def get_signals(_supabase, hours: int = 24) -> pd.DataFrame:
    """è·å–ä¿¡å·æ•°æ®"""
    cutoff = (datetime.now() - timedelta(hours=hours)).isoformat()

    result = (
        _supabase.table("analysis_signals")
        .select("*")
        .gte("created_at", cutoff)
        .order("confidence", desc=True)
        .execute()
    )

    if result.data:
        return pd.DataFrame(result.data)
    return pd.DataFrame()


@st.cache_data(ttl=300)
def get_watchlist_signals(_supabase, hours: int = 24) -> pd.DataFrame:
    """è·å–å“¨å…µå‘Šè­¦ä¿¡å·ã€‚"""
    cutoff = (datetime.now() - timedelta(hours=hours)).isoformat()
    result = (
        _supabase.table("analysis_signals")
        .select("*")
        .eq("signal_type", "watchlist_alert")
        .gte("created_at", cutoff)
        .order("created_at", desc=True)
        .limit(500)
        .execute()
    )
    if result.data:
        return pd.DataFrame(result.data)
    return pd.DataFrame()


@st.cache_data(ttl=300)
def get_entity_relations_graph(
    _supabase,
    min_confidence: float = 0.55,
    limit: int = 400,
) -> pd.DataFrame:
    """è·å–å®ä½“å…³ç³»å›¾è°±æ•°æ®ã€‚"""
    relation_rows = (
        _supabase.table("entity_relations")
        .select("*")
        .gte("confidence", min_confidence)
        .order("last_seen", desc=True)
        .limit(limit)
        .execute()
    )
    relations = relation_rows.data or []
    if not relations:
        return pd.DataFrame()

    def _as_int(value: Any) -> int:
        if isinstance(value, int):
            return value
        if isinstance(value, str) and value.isdigit():
            return int(value)
        return -1

    entity_ids = sorted(
        {
            _as_int(entity_id)
            for row in relations
            for entity_id in [row.get("entity1_id"), row.get("entity2_id")]
            if _as_int(entity_id) > 0
        }
    )
    if not entity_ids:
        return pd.DataFrame()

    entity_map: Dict[int, Dict[str, str]] = {}
    batch_size = 200
    for i in range(0, len(entity_ids), batch_size):
        batch_ids = entity_ids[i : i + batch_size]
        entity_rows = (
            _supabase.table("entities")
            .select("id,name,entity_type,category")
            .in_("id", batch_ids)
            .execute()
        )
        for row in entity_rows.data or []:
            entity_id = _as_int(row.get("id"))
            if entity_id <= 0:
                continue
            entity_map[entity_id] = {
                "name": str(row.get("name", "")),
                "entity_type": str(row.get("entity_type", "other")),
                "category": str(row.get("category", "unknown")),
            }

    normalized_rows: List[Dict[str, Any]] = []
    for row in relations:
        entity1_id = _as_int(row.get("entity1_id"))
        entity2_id = _as_int(row.get("entity2_id"))
        if entity1_id not in entity_map or entity2_id not in entity_map:
            continue
        left = entity_map[entity1_id]
        right = entity_map[entity2_id]
        normalized_rows.append(
            {
                "id": row.get("id"),
                "entity1_id": entity1_id,
                "entity2_id": entity2_id,
                "entity1_name": left["name"],
                "entity1_type": left["entity_type"],
                "entity2_name": right["name"],
                "entity2_type": right["entity_type"],
                "relation_text": row.get("relation_text", ""),
                "confidence": float(row.get("confidence", 0.0) or 0.0),
                "source_count": int(row.get("source_count", 0) or 0),
                "last_seen": row.get("last_seen"),
                "source_article_ids": row.get("source_article_ids", []),
            }
        )

    return pd.DataFrame(normalized_rows)


@st.cache_data(ttl=300)
def get_cluster_article_links(
    _supabase, cluster_ids: Tuple[int, ...], per_cluster: int = 3
) -> Dict[int, List[Dict[str, str]]]:
    """æ‰¹é‡è·å–èšç±»å…³è”æ–‡ç« åŸæ–‡é“¾æ¥"""
    if not cluster_ids:
        return {}

    relation_rows: List[Dict] = []
    id_list = [int(item) for item in cluster_ids]
    batch_size = 200
    for i in range(0, len(id_list), batch_size):
        batch_cluster_ids = id_list[i : i + batch_size]
        result = (
            _supabase.table("article_analyses")
            .select("id, cluster_id, article_id")
            .in_("cluster_id", batch_cluster_ids)
            .order("id")
            .execute()
        )
        relation_rows.extend(result.data or [])

    cluster_article_ids: Dict[int, List[int]] = {}
    for row in relation_rows:
        cluster_id = int(row.get("cluster_id"))
        article_id = int(row.get("article_id"))
        ids = cluster_article_ids.setdefault(cluster_id, [])
        if article_id not in ids and len(ids) < per_cluster:
            ids.append(article_id)

    all_article_ids = sorted(
        {article_id for ids in cluster_article_ids.values() for article_id in ids}
    )
    if not all_article_ids:
        return {}

    article_map: Dict[int, Dict[str, str]] = {}
    for i in range(0, len(all_article_ids), batch_size):
        batch_article_ids = all_article_ids[i : i + batch_size]
        result = (
            _supabase.table("articles")
            .select("id, title, url")
            .in_("id", batch_article_ids)
            .execute()
        )
        for article in result.data or []:
            article_map[int(article["id"])] = {
                "title": article.get("title", "åŸæ–‡é“¾æ¥"),
                "url": article.get("url", ""),
            }

    links_map: Dict[int, List[Dict[str, str]]] = {}
    for cluster_id, article_ids in cluster_article_ids.items():
        links: List[Dict[str, str]] = []
        for article_id in article_ids:
            article = article_map.get(article_id)
            if not article or not article.get("url"):
                continue
            links.append(article)
        links_map[cluster_id] = links

    return links_map


SIGNAL_TYPE_NAMES = {
    "velocity_spike": "ğŸš€ é€Ÿåº¦æ¿€å¢",
    "convergence": "ğŸ”„ å¤šæºèšåˆ",
    "triangulation": "ğŸ“ ä¸‰è§’éªŒè¯",
    "hotspot_escalation": "ğŸ”¥ çƒ­ç‚¹å‡çº§",
    "economic_indicator_alert": "ğŸ“Š ç»æµæŒ‡æ ‡å¼‚å¸¸",
    "natural_disaster_signal": "ğŸŒ‹ è‡ªç„¶ç¾å®³",
    "geopolitical_intensity": "ğŸŒ åœ°ç¼˜æ”¿æ²»ç´§å¼ ",
    "watchlist_alert": "ğŸ›°ï¸ åœºæ™¯å“¨å…µå‘Šè­¦",
}

WATCHLIST_LEVEL_COLORS = {
    "L1": "#67f7c2",
    "L2": "#ffd166",
    "L3": "#ff9f43",
    "L4": "#ff6b7a",
}


def parse_json_field(value: Any, expected_type: type):
    """è§£æ JSON å­—æ®µï¼ˆå…¼å®¹å¯¹è±¡å’Œå­—ç¬¦ä¸²ï¼‰ã€‚"""
    if isinstance(value, expected_type):
        return value
    if isinstance(value, str) and value.strip():
        try:
            parsed = json.loads(value)
            if isinstance(parsed, expected_type):
                return parsed
        except Exception:
            return expected_type()
    return expected_type()


def parse_string_list(value: Any) -> List[str]:
    """è§£æå­—ç¬¦ä¸²åˆ—è¡¨å­—æ®µã€‚"""
    parsed = parse_json_field(value, list)
    if isinstance(parsed, list):
        return [str(item).strip() for item in parsed if str(item).strip()]
    return []


def get_signal_name(row: pd.Series) -> str:
    """è·å–ä¿¡å·å±•ç¤ºåç§°"""
    signal_name = row.get("name")
    if signal_name and signal_name != "N/A":
        return signal_name
    signal_type = row.get("signal_type", "unknown")
    return SIGNAL_TYPE_NAMES.get(signal_type, f"âš¡ {signal_type}")


def parse_signal_explanation(row: pd.Series) -> dict:
    """è§£æä¿¡å·è§£é‡Šä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨ LLM/æ•°æ®åº“ä¸­çš„ç»“æ„åŒ–è§£é‡Š"""
    confidence = float(row.get("confidence", 0) or 0)
    signal_type = row.get("signal_type", "unknown")
    details = {}

    raw_rationale = row.get("rationale")
    if isinstance(raw_rationale, dict):
        details = raw_rationale
    elif isinstance(raw_rationale, str) and raw_rationale.strip():
        try:
            parsed = json.loads(raw_rationale)
            if isinstance(parsed, dict):
                details = parsed
        except Exception:
            details = {}

    parsed_details = (
        details.get("details") if isinstance(details.get("details"), dict) else details
    )
    related_events = details.get("related_events", []) if isinstance(details, dict) else []
    if not isinstance(related_events, list):
        related_events = []

    # å…¼å®¹åç»­å¯èƒ½æ¥å…¥çš„ LLM è§£é‡Šå­—æ®µ
    if any(k in details for k in ["importance", "actionable", "confidence_reason"]):
        return {
            "why": details.get("importance", row.get("description", "æš‚æ— è§¦å‘åŸå› ")),
            "meaning": details.get("meaning", row.get("description", "æš‚æ— å«ä¹‰è§£é‡Š")),
            "action": details.get("actionable", "å»ºè®®ç»§ç»­è§‚å¯Ÿåç»­å˜åŒ–"),
            "confidence_reason": details.get(
                "confidence_reason", f"å½“å‰ç³»ç»Ÿè¯„åˆ†ç½®ä¿¡åº¦ä¸º {confidence:.2f}"
            ),
            "events": related_events,
            "alert_level": details.get("alert_level"),
        }

    def _format_source_types(value) -> str:
        if isinstance(value, list):
            return ", ".join(str(v) for v in value if v)
        if value:
            return str(value)
        return "N/A"

    if signal_type == "velocity_spike":
        cluster_count = parsed_details.get("cluster_count", "N/A")
        threshold = parsed_details.get("threshold", "N/A")
        window_hours = parsed_details.get("time_window_hours", 1)
        why = f"{window_hours}å°æ—¶å†…èšç±»æ•°è¾¾åˆ° {cluster_count}ï¼Œè¶…è¿‡é˜ˆå€¼ {threshold}"
        meaning = "ä»£è¡¨çŸ­æ—¶é—´å†…ç›¸å…³æ–°é—»å¯†åº¦ä¸Šå‡ï¼Œäº‹ä»¶å¯èƒ½è¿›å…¥å¿«é€Ÿå‘é…µé˜¶æ®µã€‚"
        action = "å»ºè®®ä¼˜å…ˆè·Ÿè¸ªè¯¥æ—¶æ®µæ–°å¢èšç±»ï¼Œè§‚å¯Ÿæ˜¯å¦å‡ºç°è·¨ä¸»é¢˜æ‰©æ•£ã€‚"
    elif signal_type == "convergence":
        source_count = parsed_details.get("source_count", "N/A")
        source_types = _format_source_types(parsed_details.get("source_types", []))
        why = f"åŒä¸€äº‹ä»¶è¢« {source_count} ç±»æ¥æºåŒæ—¶æŠ¥é“ï¼ˆ{source_types}ï¼‰"
        meaning = "ä»£è¡¨äº‹ä»¶å¯éªŒè¯æ€§ä¸Šå‡ï¼Œå•ä¸€æ¥æºåå·®é£é™©ä¸‹é™ã€‚"
        action = "å»ºè®®é‡ç‚¹æŸ¥çœ‹æ¥æºå·®å¼‚ï¼Œç¡®è®¤å…³é”®äº‹å®æ˜¯å¦ä¸€è‡´ã€‚"
    elif signal_type == "triangulation":
        source_types = _format_source_types(parsed_details.get("source_types", []))
        why = f"å·²å‡ºç°å¤šç±»å…³é”®æ¥æºäº¤å‰éªŒè¯ï¼ˆ{source_types}ï¼‰"
        meaning = "ä»£è¡¨ä¿¡å·å¯é æ€§é«˜ï¼Œäº‹ä»¶çœŸå®æ€§é€šå¸¸æ›´å¼ºã€‚"
        action = "å»ºè®®å°†è¯¥ç±»ä¿¡å·ä½œä¸ºé‡ç‚¹é¢„è­¦è¾“å…¥ã€‚"
    elif signal_type == "hotspot_escalation":
        level = parsed_details.get("escalation_level", "unknown")
        score = parsed_details.get("total_score", "N/A")
        article_count = parsed_details.get("article_count", "N/A")
        why = f"å‡çº§ç­‰çº§ {level}ï¼Œæ€»è¯„åˆ† {score}ï¼Œèšç±»æ–‡ç« æ•° {article_count}"
        meaning = "ä»£è¡¨äº‹ä»¶çƒ­åº¦å’Œå½±å“é¢æ­£åœ¨æŠ¬å‡ï¼Œåç»­å¯èƒ½å‡çº§ã€‚"
        action = "å»ºè®®ç»“åˆå®ä½“è¶‹åŠ¿ä¸æ¥æºå˜åŒ–ï¼ŒæŒç»­å¤æ ¸å‡çº§æ–¹å‘ã€‚"
    elif signal_type == "watchlist_alert":
        sentinel_name = parsed_details.get("sentinel_name", row.get("description", "åœºæ™¯å“¨å…µ"))
        level = row.get("alert_level") or parsed_details.get("alert_level", "L1")
        risk_score = row.get("risk_score") or parsed_details.get("risk_score", "N/A")
        trigger_reasons = parsed_details.get("trigger_reasons", [])
        why = f"{sentinel_name} å½“å‰ç­‰çº§ {level}ï¼Œé£é™©åˆ† {risk_score}"
        if isinstance(trigger_reasons, list) and trigger_reasons:
            meaning = "ï¼›".join([str(item) for item in trigger_reasons[:3]])
        else:
            meaning = "å·²å‘½ä¸­åœºæ™¯è§„åˆ™ï¼Œå»ºè®®å…³æ³¨æ¥æºæ”¶æ•›ä¸å®˜æ–¹ç¡®è®¤ã€‚"
        action = parsed_details.get("suggested_action", "å»ºè®®å¿«é€Ÿå¤æ ¸å¹¶æ›´æ–°å“¨å…µæ€åŠ¿ã€‚")
    else:
        why = row.get("description", "æš‚æ— è§¦å‘åŸå› ")
        meaning = "ä»£è¡¨ç³»ç»Ÿæ£€æµ‹åˆ°å€¼å¾—å…³æ³¨çš„å¼‚å¸¸å˜åŒ–ã€‚"
        action = "å»ºè®®ç»“åˆä¸Šä¸‹æ–‡è¿›ä¸€æ­¥äººå·¥å¤æ ¸ã€‚"

    return {
        "why": why,
        "meaning": meaning,
        "action": action,
        "confidence_reason": f"å½“å‰ç³»ç»Ÿè¯„åˆ†ç½®ä¿¡åº¦ä¸º {confidence:.2f}",
        "events": related_events,
        "alert_level": parsed_details.get("alert_level"),
    }


def format_related_events(events: list, limit: int = 2) -> str:
    """æ ¼å¼åŒ–å…³è”äº‹ä»¶æ ‡é¢˜åˆ—è¡¨"""
    if not events:
        return ""
    titles = []
    for event in events[:limit]:
        if isinstance(event, dict) and event.get("title"):
            titles.append(str(event["title"]))
    return "ï¼›".join(titles)


def get_related_event_links(
    events: list, cluster_links_map: Dict[int, List[Dict[str, str]]]
) -> List[Dict[str, str]]:
    """å°†ä¿¡å·å…³è”äº‹ä»¶è½¬æ¢ä¸ºå¯ç‚¹å‡»é“¾æ¥ä¿¡æ¯"""
    results: List[Dict[str, str]] = []
    for event in events:
        if not isinstance(event, dict):
            continue
        title = str(event.get("title") or "å…³è”äº‹ä»¶").strip()
        cluster_id_raw = event.get("cluster_id")
        cluster_id = None
        if isinstance(cluster_id_raw, int):
            cluster_id = cluster_id_raw
        elif isinstance(cluster_id_raw, str) and cluster_id_raw.isdigit():
            cluster_id = int(cluster_id_raw)

        url = ""
        if cluster_id is not None:
            candidates = cluster_links_map.get(cluster_id, [])
            if candidates:
                url = candidates[0].get("url", "")

        results.append({"title": title, "url": url})
    return results


def render_external_link(label: str, url: str):
    """æ¸²æŸ“å¤–éƒ¨é“¾æ¥ï¼Œå…¼å®¹ä¸æ”¯æŒ key å‚æ•°çš„æ—§ç‰ˆ Streamlit"""
    if not url:
        return
    safe_label = html.escape(label)
    safe_url = html.escape(url, quote=True)
    st.markdown(
        f'<a href="{safe_url}" target="_blank" rel="noopener noreferrer">ğŸ”— {safe_label}</a>',
        unsafe_allow_html=True,
    )


def short_text(text: str, max_len: int = 80) -> str:
    """å‹ç¼©æ–‡æœ¬ï¼Œä¾¿äºåœ¨åˆ—è¡¨ä¸­å¿«é€Ÿæ‰«æ"""
    if not text:
        return ""
    cleaned = " ".join(str(text).split())
    if len(cleaned) <= max_len:
        return cleaned
    return f"{cleaned[:max_len]}..."


def normalize_watchlist_record(row: pd.Series) -> Dict[str, Any]:
    """æ ‡å‡†åŒ–å“¨å…µè®°å½•ï¼Œä¾¿äºé¡µé¢æ¸²æŸ“ã€‚"""
    details = parse_json_field(row.get("details"), dict)
    trigger_reasons = parse_string_list(
        row.get("trigger_reasons", details.get("trigger_reasons", []))
    )
    evidence_links = parse_string_list(
        row.get("evidence_links", details.get("evidence_links", []))
    )
    related_entities = parse_string_list(details.get("related_entities", []))

    created_at = str(row.get("created_at", ""))
    sentinel_name = str(
        details.get("sentinel_name")
        or row.get("name")
        or row.get("description")
        or "åœºæ™¯å“¨å…µ"
    )

    return {
        "signal_key": str(row.get("signal_key", "")),
        "sentinel_id": str(
            row.get("sentinel_id") or details.get("sentinel_id") or "unknown"
        ),
        "sentinel_name": sentinel_name,
        "alert_level": str(row.get("alert_level") or details.get("alert_level") or "L1"),
        "risk_score": float(row.get("risk_score") or details.get("risk_score") or 0.0),
        "confidence": float(row.get("confidence") or 0.0),
        "trigger_reasons": trigger_reasons,
        "evidence_links": evidence_links,
        "related_entities": related_entities,
        "suggested_action": str(details.get("suggested_action") or "å»ºè®®äººå·¥å¤æ ¸ã€‚"),
        "next_review_time": str(details.get("next_review_time") or ""),
        "description": str(row.get("description") or ""),
        "created_at": created_at,
    }


@st.cache_data(ttl=300)
def get_stats(_supabase) -> dict:
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    today = datetime.now().strftime("%Y-%m-%d")

    # ä»Šæ—¥èšç±»æ•°
    clusters_today = (
        _supabase.table("analysis_clusters")
        .select("*", count="exact")
        .gte("created_at", today)
        .execute()
    )

    # ä»Šæ—¥ä¿¡å·æ•°
    signals_today = (
        _supabase.table("analysis_signals")
        .select("*", count="exact")
        .gte("created_at", today)
        .execute()
    )

    # æ€»æ–‡ç« æ•°
    articles_total = _supabase.table("articles").select("*", count="exact").execute()

    # æœªåˆ†ææ–‡ç« æ•°
    articles_unanalyzed = (
        _supabase.table("articles")
        .select("*", count="exact")
        .is_("analyzed_at", "null")
        .execute()
    )

    return {
        "clusters_today": clusters_today.count,
        "signals_today": signals_today.count,
        "articles_total": articles_total.count,
        "articles_unanalyzed": articles_unanalyzed.count,
    }


# ä¾§è¾¹æ 
def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ """
    st.sidebar.markdown("## ğŸ“Š å¯¼èˆª")

    page = st.sidebar.radio(
        "é€‰æ‹©é¡µé¢:",
        [
            "ğŸ  æ¦‚è§ˆé¦–é¡µ",
            "ğŸ›°ï¸ å“¨å…µæ€åŠ¿",
            "ğŸ•¸ï¸ å…³ç³»å›¾è°±",
            "ğŸ”¥ çƒ­ç‚¹è¯¦æƒ…",
            "ğŸ“¡ ä¿¡å·ä¸­å¿ƒ",
            "ğŸ“ å®ä½“æ¡£æ¡ˆ",
            "ğŸ“ˆ æ•°æ®ç»Ÿè®¡",
        ],
    )

    st.sidebar.markdown("---")
    st.sidebar.markdown("### âš™ï¸ è®¾ç½®")

    time_range = st.sidebar.selectbox("æ—¶é—´èŒƒå›´:", ["24å°æ—¶", "7å¤©", "30å¤©"], index=0)

    hours_map = {"24å°æ—¶": 24, "7å¤©": 168, "30å¤©": 720}

    category = st.sidebar.selectbox(
        "åˆ†ç±»ç­›é€‰:", ["å…¨éƒ¨", "military", "politics", "economy", "tech"]
    )

    return page, hours_map[time_range], category


# æ¦‚è§ˆé¦–é¡µ
def render_overview(supabase, hours: int, category: str):
    """æ¸²æŸ“æ¦‚è§ˆé¦–é¡µ"""
    st.markdown(
        '<div class="main-header">ğŸ‡ºğŸ‡¸ US-Monitor çƒ­ç‚¹åˆ†æ</div>', unsafe_allow_html=True
    )

    # è·å–ç»Ÿè®¡æ•°æ®
    stats = get_stats(supabase)

    # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("ä»Šæ—¥èšç±»", stats["clusters_today"])
    with col2:
        st.metric("ä»Šæ—¥ä¿¡å·", stats["signals_today"])
    with col3:
        st.metric("æ€»æ–‡ç« æ•°", f"{stats['articles_total']:,}")
    with col4:
        st.metric("å¾…åˆ†æ", stats["articles_unanalyzed"])

    st.markdown("---")

    # è·å–æ•°æ®
    clusters_df = get_clusters(supabase, hours, category, only_hot=True)
    signals_df = get_signals(supabase, hours)

    # æœ€æ–°çƒ­ç‚¹
    st.markdown("### ğŸ”¥ æœ€æ–°çƒ­ç‚¹ (TOP 5)")

    if clusters_df.empty:
        st.info("æš‚æ— çƒ­ç‚¹æ•°æ®")
    else:
        top_clusters = clusters_df.head(5)
        top_cluster_ids = tuple(
            int(cluster_id)
            for cluster_id in top_clusters["id"].tolist()
            if pd.notna(cluster_id)
        )
        top_links_map = get_cluster_article_links(
            supabase, top_cluster_ids, per_cluster=1
        )

        for idx, row in top_clusters.iterrows():
            with st.container():
                st.markdown(
                    f"""
                <div class="hotspot-card">
                    <h4>{row.get("primary_title", "N/A")}</h4>
                    <p><strong>ä¸­æ–‡æ‘˜è¦:</strong> {row.get("summary", "N/A")[:150]}...</p>
                    <p class="meta-text">
                        ğŸ“ {row.get("category", "N/A")} |
                        ğŸ“„ {row.get("article_count", 0)} ç¯‡æ–‡ç«  |
                        â° {row.get("created_at", "N/A")[:10]}
                    </p>
                </div>
                """,
                    unsafe_allow_html=True,
                )

                cluster_id = int(row["id"]) if pd.notna(row.get("id")) else None
                primary_link = row.get("primary_link")
                if not primary_link and cluster_id:
                    candidates = top_links_map.get(cluster_id, [])
                    primary_link = candidates[0]["url"] if candidates else ""

                if primary_link:
                    render_external_link("æŸ¥çœ‹è‹±æ–‡åŸæ–‡", primary_link)

    # æœ€æ–°ä¿¡å·
    st.markdown("### ğŸ“¡ æœ€æ–°ä¿¡å·")

    if signals_df.empty:
        st.info("æš‚æ— ä¿¡å·æ•°æ®")
    else:
        for idx, row in signals_df.head(5).iterrows():
            confidence = row.get("confidence", 0)
            explanation = parse_signal_explanation(row)
            alert_level = str(
                row.get("alert_level") or explanation.get("alert_level") or ""
            ).strip().upper()
            if alert_level in {"L4", "L3"}:
                level_class = "signal-high"
                level_text = alert_level
            elif alert_level == "L2":
                level_class = "signal-medium"
                level_text = alert_level
            elif alert_level == "L1":
                level_class = "signal-low"
                level_text = alert_level
            elif confidence >= 0.8:
                level_class = "signal-high"
                level_text = "é«˜"
            elif confidence >= 0.6:
                level_class = "signal-medium"
                level_text = "ä¸­"
            else:
                level_class = "signal-low"
                level_text = "ä½"

            signal_name = get_signal_name(row)
            event_text = format_related_events(explanation.get("events", []), limit=2)
            compact_why = short_text(explanation.get("why", ""), 68)
            compact_meaning = short_text(explanation.get("meaning", ""), 68)
            compact_events = short_text(event_text or "æ— å¯ç”¨å…³è”äº‹ä»¶", 56)
            created_at = str(row.get("created_at", "N/A"))[:16]

            st.markdown(
                f"""
            <div class="hotspot-card">
                <h5>
                    {html.escape(str(row.get("icon", "âš¡")))} {html.escape(signal_name)}
                    <span class="signal-badge {level_class}">{level_text}</span>
                </h5>
                <p class="meta-text">
                    ç½®ä¿¡åº¦: {confidence:.2f} | æ—¶é—´: {html.escape(created_at)}
                </p>
                <p><strong>åŸå› :</strong> {html.escape(compact_why)}</p>
                <p><strong>å«ä¹‰:</strong> {html.escape(compact_meaning)}</p>
                <p class="meta-text">å…³è”: {html.escape(compact_events)}</p>
            </div>
            """,
                unsafe_allow_html=True,
            )


# çƒ­ç‚¹è¯¦æƒ…é¡µ
def render_hotspots(supabase, hours: int, category: str):
    """æ¸²æŸ“çƒ­ç‚¹è¯¦æƒ…é¡µ"""
    st.markdown('<div class="main-header">ğŸ”¥ çƒ­ç‚¹è¯¦æƒ…</div>', unsafe_allow_html=True)

    clusters_df = get_clusters(supabase, hours, category, only_hot=True)

    if clusters_df.empty:
        st.info("æš‚æ— çƒ­ç‚¹æ•°æ®")
        return

    cluster_ids = tuple(
        int(cluster_id)
        for cluster_id in clusters_df["id"].tolist()
        if pd.notna(cluster_id)
    )
    links_map = get_cluster_article_links(supabase, cluster_ids, per_cluster=3)

    # åˆ†ç±»æ ‡ç­¾
    tabs = st.tabs(["å…¨éƒ¨", "å†›äº‹", "æ”¿æ²»", "ç»æµ"])
    categories = [None, "military", "politics", "economy"]

    for tab, cat in zip(tabs, categories):
        with tab:
            if cat:
                filtered_df = clusters_df[clusters_df["category"] == cat]
            else:
                filtered_df = clusters_df

            st.write(f"å…± {len(filtered_df)} ä¸ªçƒ­ç‚¹")

            for idx, row in filtered_df.iterrows():
                with st.expander(f"ğŸ“° {row.get('primary_title', 'N/A')}"):
                    st.markdown(f"**ä¸­æ–‡æ‘˜è¦:**")
                    st.write(row.get("summary", "N/A"))

                    st.markdown(f"**å…³é”®å®ä½“:**")
                    try:
                        raw_entities = row.get("key_entities", "[]")
                        entities = (
                            raw_entities
                            if isinstance(raw_entities, list)
                            else json.loads(raw_entities)
                        )
                        if entities:
                            st.write(", ".join(entities))
                        else:
                            st.write("æ— ")
                    except Exception:
                        st.write("æ— ")

                    st.markdown(f"**å½±å“åˆ†æ:**")
                    st.write(row.get("impact", "æš‚æ— åˆ†æ"))

                    st.markdown(f"**è¶‹åŠ¿åˆ¤æ–­:**")
                    st.write(row.get("trend", "æš‚æ— åˆ¤æ–­"))

                    col1, col2 = st.columns(2)
                    with col1:
                        st.write(f"ğŸ“ åˆ†ç±»: {row.get('category', 'N/A')}")
                        st.write(f"ğŸ“„ æ–‡ç« æ•°: {row.get('article_count', 0)}")
                    with col2:
                        st.write(f"â° åˆ›å»ºæ—¶é—´: {row.get('created_at', 'N/A')[:16]}")

                    cluster_id = int(row["id"]) if pd.notna(row.get("id")) else None
                    primary_link = row.get("primary_link")
                    article_links = links_map.get(cluster_id, []) if cluster_id else []
                    if not primary_link and article_links:
                        primary_link = article_links[0]["url"]

                    if primary_link:
                        render_external_link("æŸ¥çœ‹ä¸»åŸæ–‡", primary_link)

                    if article_links:
                        st.markdown("**ç›¸å…³æ–°é—»åŸæ–‡:**")
                        for link_idx, link in enumerate(article_links[:3], 1):
                            title = (link.get("title") or "åŸæ–‡é“¾æ¥").strip()[:80]
                            render_external_link(
                                f"åŸæ–‡{link_idx}: {title}",
                                link.get("url", ""),
                            )


# ä¿¡å·ä¸­å¿ƒé¡µ
def render_signals(supabase, hours: int):
    """æ¸²æŸ“ä¿¡å·ä¸­å¿ƒé¡µ"""
    st.markdown('<div class="main-header">ğŸ“¡ ä¿¡å·ä¸­å¿ƒ</div>', unsafe_allow_html=True)

    signals_df = get_signals(supabase, hours)

    if signals_df.empty:
        st.info("æš‚æ— ä¿¡å·æ•°æ®")
        return

    # ä¿¡å·ç±»å‹ç­›é€‰
    signal_types = signals_df["signal_type"].unique().tolist()
    selected_type = st.selectbox("ä¿¡å·ç±»å‹:", ["å…¨éƒ¨"] + signal_types)

    if selected_type != "å…¨éƒ¨":
        signals_df = signals_df[signals_df["signal_type"] == selected_type]

    # ç½®ä¿¡åº¦ç­›é€‰
    min_confidence = st.slider("æœ€å°ç½®ä¿¡åº¦:", 0.0, 1.0, 0.5, 0.1)
    signals_df = signals_df[signals_df["confidence"] >= min_confidence]

    col_a, col_b = st.columns([1, 1])
    with col_a:
        view_mode = st.radio(
            "å±•ç¤ºæ¨¡å¼:",
            ["ç²¾ç®€", "è¯¦ç»†"],
            horizontal=True,
            index=0,
        )
    with col_b:
        max_per_type = st.slider("åŒç±»å‹æœ€å¤šæ˜¾ç¤º:", 1, 20, 5, 1)

    if "signal_type" in signals_df.columns:
        type_counts = signals_df["signal_type"].value_counts()
        if not type_counts.empty:
            dominant_type = type_counts.index[0]
            dominant_count = int(type_counts.iloc[0])
            if dominant_count >= 10 and dominant_count >= len(signals_df) * 0.7:
                st.warning(
                    f"å½“å‰ä¿¡å·é«˜åº¦é›†ä¸­åœ¨ `{dominant_type}`ï¼ˆ{dominant_count}/{len(signals_df)}ï¼‰ã€‚"
                    "å·²æŒ‰åŒç±»å‹ä¸Šé™åšå‹ç¼©å±•ç¤ºã€‚"
                )
        signals_df = (
            signals_df.sort_values("confidence", ascending=False)
            .groupby("signal_type", group_keys=False)
            .head(max_per_type)
            .reset_index(drop=True)
        )

    st.write(f"å½“å‰å±•ç¤º {len(signals_df)} ä¸ªä¿¡å·")

    parsed_signals: List[Tuple[int, pd.Series, Dict]] = []
    related_cluster_ids = set()
    for idx, row in signals_df.iterrows():
        explanation = parse_signal_explanation(row)
        parsed_signals.append((idx, row, explanation))
        for event in explanation.get("events", []):
            if not isinstance(event, dict):
                continue
            cluster_id_raw = event.get("cluster_id")
            if isinstance(cluster_id_raw, int):
                related_cluster_ids.add(cluster_id_raw)
            elif isinstance(cluster_id_raw, str) and cluster_id_raw.isdigit():
                related_cluster_ids.add(int(cluster_id_raw))

    related_links_map = get_cluster_article_links(
        supabase, tuple(sorted(related_cluster_ids)), per_cluster=1
    )

    # æ˜¾ç¤ºä¿¡å·åˆ—è¡¨
    for idx, row, explanation in parsed_signals:
        confidence = row.get("confidence", 0)

        if confidence >= 0.8:
            level_color = "#ff4b4b"
        elif confidence >= 0.6:
            level_color = "#ffa500"
        else:
            level_color = "#4caf50"

        signal_name = get_signal_name(row)
        event_text = format_related_events(explanation.get("events", []), limit=3)

        if view_mode == "ç²¾ç®€":
            compact_event = short_text(event_text or "æ— å¯ç”¨å…³è”äº‹ä»¶", 72)
            compact_reason = short_text(explanation["why"], 90)
            compact_meaning = short_text(explanation["meaning"], 90)
            st.markdown(
                f"""
            <div class="hotspot-card" style="border-left: 4px solid {level_color};">
                <h4>{row.get("icon", "âš¡")} {signal_name}</h4>
                <p><strong>äº‹ä»¶:</strong> {compact_event}</p>
                <p><strong>è§¦å‘:</strong> {compact_reason}</p>
                <p><strong>å«ä¹‰:</strong> {compact_meaning}</p>
                <p class="meta-text">
                    ç½®ä¿¡åº¦: {confidence:.2f} | æ—¶é—´: {row.get("created_at", "N/A")[:16]}
                </p>
            </div>
            """,
                unsafe_allow_html=True,
            )
        else:
            st.markdown(
                f"""
            <div class="hotspot-card" style="border-left: 4px solid {level_color};">
                <h4>{row.get("icon", "âš¡")} {signal_name}</h4>
                <p><strong>è§¦å‘åŸå› :</strong> {explanation["why"]}</p>
                <p><strong>ä»£è¡¨å«ä¹‰:</strong> {explanation["meaning"]}</p>
                <p><strong>å»ºè®®åŠ¨ä½œ:</strong> {explanation["action"]}</p>
                <p><strong>å…³è”äº‹ä»¶:</strong> {event_text or "æ— å¯ç”¨å…³è”äº‹ä»¶"}</p>
                <p>
                    <span style="color: {level_color}; font-weight: bold;">
                        ç½®ä¿¡åº¦: {confidence:.2f}
                    </span> |
                    <span class="meta-text">æ—¶é—´: {row.get("created_at", "N/A")[:16]}</span> |
                    <span class="meta-text">ä¾æ®: {explanation["confidence_reason"]}</span>
                </p>
            </div>
            """,
                unsafe_allow_html=True,
            )

        event_links = get_related_event_links(
            explanation.get("events", []), related_links_map
        )
        if event_links:
            st.markdown("**å…³è”äº‹ä»¶åŸæ–‡:**")
            for link_idx, event_link in enumerate(event_links[:3], 1):
                event_title = event_link.get("title", "å…³è”äº‹ä»¶åŸæ–‡")[:80]
                event_url = event_link.get("url", "")
                if event_url:
                    render_external_link(f"äº‹ä»¶{link_idx}: {event_title}", event_url)
                else:
                    st.write(f"- {event_title}ï¼ˆæš‚æ— åŸæ–‡é“¾æ¥ï¼‰")

    # ä¿¡å·ç»Ÿè®¡å›¾è¡¨
    if not signals_df.empty:
        st.markdown("### ğŸ“Š ä¿¡å·ç»Ÿè®¡")

        col1, col2 = st.columns(2)

        with col1:
            # æŒ‰ç±»å‹ç»Ÿè®¡
            type_counts = signals_df["signal_type"].value_counts()
            st.bar_chart(type_counts)

        with col2:
            # æŒ‰ç½®ä¿¡åº¦åˆ†å¸ƒ
            conf_dist = pd.cut(
                signals_df["confidence"],
                bins=[0, 0.6, 0.8, 1.0],
                labels=["ä½", "ä¸­", "é«˜"],
            ).value_counts()
            st.bar_chart(conf_dist)


def render_monitor(supabase, hours: int):
    """æ¸²æŸ“å“¨å…µæ€åŠ¿é¡µã€‚"""
    st.markdown('<div class="main-header">ğŸ›°ï¸ å“¨å…µæ€åŠ¿</div>', unsafe_allow_html=True)

    watchlist_df = get_watchlist_signals(supabase, hours)
    if watchlist_df.empty:
        st.info("æœ€è¿‘æ—¶é—´çª—å£å†…æš‚æ— å“¨å…µå‘Šè­¦ã€‚")
        return

    records = [normalize_watchlist_record(row) for _, row in watchlist_df.iterrows()]
    records_df = pd.DataFrame(records)
    records_df["created_dt"] = pd.to_datetime(records_df["created_at"], errors="coerce")
    records_df = records_df.sort_values("created_dt", ascending=False)

    l34_count = int(records_df["alert_level"].isin(["L3", "L4"]).sum())
    sentinel_count = int(records_df["sentinel_id"].nunique())
    latest_time = records_df["created_dt"].max()
    latest_text = "N/A" if pd.isna(latest_time) else str(latest_time)[:16]

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("å‘Šè­¦æ€»æ•°", len(records_df))
    with col2:
        st.metric("L3/L4", l34_count)
    with col3:
        st.metric("å“¨å…µæ•°é‡", sentinel_count)
    with col4:
        st.metric("æœ€æ–°å‘Šè­¦", latest_text)

    levels = ["L1", "L2", "L3", "L4"]
    selected_levels = st.multiselect("ç­‰çº§ç­›é€‰", levels, default=levels)
    sentinel_options = ["å…¨éƒ¨"] + sorted(records_df["sentinel_name"].dropna().unique().tolist())
    selected_sentinel = st.selectbox("å“¨å…µç­›é€‰", sentinel_options)

    filtered_df = records_df[records_df["alert_level"].isin(selected_levels)]
    if selected_sentinel != "å…¨éƒ¨":
        filtered_df = filtered_df[filtered_df["sentinel_name"] == selected_sentinel]

    if filtered_df.empty:
        st.info("ç­›é€‰æ¡ä»¶ä¸‹æš‚æ— å‘Šè­¦ã€‚")
        return

    st.markdown("### åœºæ™¯æ€åŠ¿å¡")
    latest_df = (
        filtered_df.sort_values("created_dt", ascending=False)
        .drop_duplicates(subset=["sentinel_id"])
        .reset_index(drop=True)
    )

    columns = st.columns(2)
    for idx, (_, row) in enumerate(latest_df.iterrows()):
        level = str(row.get("alert_level", "L1")).upper()
        card_color = WATCHLIST_LEVEL_COLORS.get(level, "#8ea0bf")
        with columns[idx % 2]:
            st.markdown(
                f"""
<div class="hotspot-card" style="border-left: 4px solid {card_color};">
  <h4>{html.escape(str(row.get("sentinel_name", "åœºæ™¯å“¨å…µ")))} Â· {level}</h4>
  <p><strong>é£é™©åˆ†:</strong> {float(row.get("risk_score", 0.0)):.2f}</p>
  <p class="meta-text">æ—¶é—´: {str(row.get("created_at", "N/A"))[:16]}</p>
  <p><strong>å»ºè®®åŠ¨ä½œ:</strong> {html.escape(str(row.get("suggested_action", "")))}</p>
</div>
""",
                unsafe_allow_html=True,
            )
            reasons = row.get("trigger_reasons", [])
            if isinstance(reasons, list) and reasons:
                st.markdown("**è§¦å‘åŸå› **")
                for reason in reasons[:4]:
                    st.write(f"- {reason}")
            entities = row.get("related_entities", [])
            if isinstance(entities, list) and entities:
                st.caption(f"ç›¸å…³å®ä½“: {', '.join([str(item) for item in entities[:8]])}")
            next_review = str(row.get("next_review_time", "")).strip()
            if next_review:
                st.caption(f"ä¸‹æ¬¡å¤æ ¸: {next_review[:16]}")
            links = row.get("evidence_links", [])
            if isinstance(links, list) and links:
                for link_idx, link in enumerate(links[:3], 1):
                    render_external_link(f"è¯æ®é“¾æ¥ {link_idx}", str(link))

    st.markdown("### ç­‰çº§åˆ†å¸ƒ")
    level_counts = (
        filtered_df["alert_level"].value_counts().reindex(["L4", "L3", "L2", "L1"]).fillna(0)
    )
    st.bar_chart(level_counts)

    st.markdown("### å‘Šè­¦æ˜ç»†")
    detail_df = filtered_df[
        [
            "created_at",
            "sentinel_name",
            "alert_level",
            "risk_score",
            "confidence",
            "description",
        ]
    ].rename(
        columns={
            "created_at": "æ—¶é—´",
            "sentinel_name": "å“¨å…µ",
            "alert_level": "ç­‰çº§",
            "risk_score": "é£é™©åˆ†",
            "confidence": "ç½®ä¿¡åº¦",
            "description": "æ‘˜è¦",
        }
    )
    st.dataframe(detail_df, use_container_width=True, hide_index=True)


def render_graph(supabase, hours: int):
    """æ¸²æŸ“å®ä½“å…³ç³»å›¾è°±é¡µã€‚"""
    st.markdown('<div class="main-header">ğŸ•¸ï¸ å…³ç³»å›¾è°±</div>', unsafe_allow_html=True)

    col1, col2, col3 = st.columns(3)
    with col1:
        min_confidence = st.slider("æœ€å°ç½®ä¿¡åº¦", 0.3, 1.0, 0.55, 0.05)
    with col2:
        min_source_count = st.slider("æœ€å°æ¥æºæ•°", 1, 10, 1, 1)
    with col3:
        limit = st.slider("æœ€å¤§å…³ç³»æ•°", 100, 800, 400, 50)

    search_term = st.text_input("æœç´¢å®ä½“/å…³ç³»å…³é”®è¯", "")

    graph_df = get_entity_relations_graph(
        supabase,
        min_confidence=min_confidence,
        limit=limit,
    )
    if graph_df.empty:
        st.info("æš‚æ— å¯å±•ç¤ºçš„å®ä½“å…³ç³»æ•°æ®ã€‚")
        return

    graph_df = graph_df[graph_df["source_count"] >= min_source_count]
    if search_term.strip():
        pattern = search_term.strip().lower()
        graph_df = graph_df[
            graph_df["entity1_name"].str.lower().str.contains(pattern, na=False)
            | graph_df["entity2_name"].str.lower().str.contains(pattern, na=False)
            | graph_df["relation_text"].str.lower().str.contains(pattern, na=False)
        ]

    watchlist_df = get_watchlist_signals(supabase, hours=72)
    watchlist_entities = set()
    for _, row in watchlist_df.iterrows():
        normalized = normalize_watchlist_record(row)
        for entity in normalized.get("related_entities", []):
            watchlist_entities.add(str(entity).strip().lower())

    graph_df["watchlist_related"] = graph_df.apply(
        lambda row: (
            str(row.get("entity1_name", "")).lower() in watchlist_entities
            or str(row.get("entity2_name", "")).lower() in watchlist_entities
        ),
        axis=1,
    )
    only_watchlist_related = st.checkbox("ä»…æ˜¾ç¤ºå“¨å…µç›¸å…³å…³ç³»", value=False)
    if only_watchlist_related:
        graph_df = graph_df[graph_df["watchlist_related"]]

    if graph_df.empty:
        st.info("ç­›é€‰æ¡ä»¶ä¸‹æš‚æ— å…³ç³»æ•°æ®ã€‚")
        return

    unique_entities = pd.unique(
        pd.concat([graph_df["entity1_name"], graph_df["entity2_name"]], ignore_index=True)
    )
    avg_conf = float(graph_df["confidence"].mean() or 0.0)
    col_a, col_b, col_c = st.columns(3)
    with col_a:
        st.metric("å…³ç³»æ•°é‡", len(graph_df))
    with col_b:
        st.metric("å®ä½“æ•°é‡", len(unique_entities))
    with col_c:
        st.metric("å¹³å‡ç½®ä¿¡åº¦", f"{avg_conf:.2f}")

    st.markdown("### å…³ç³»è¡¨")
    table_df = graph_df[
        [
            "entity1_name",
            "entity1_type",
            "relation_text",
            "entity2_name",
            "entity2_type",
            "confidence",
            "source_count",
            "watchlist_related",
        ]
    ].sort_values(["watchlist_related", "confidence", "source_count"], ascending=False)
    st.dataframe(table_df, use_container_width=True, hide_index=True)

    st.markdown("### å›¾è°±é¢„è§ˆ")
    preview_df = graph_df.sort_values(
        ["watchlist_related", "source_count", "confidence"],
        ascending=False,
    ).head(80)

    def _dot_escape(text: str) -> str:
        return str(text or "").replace("\\", "\\\\").replace('"', '\\"')

    node_types: Dict[str, str] = {}
    for _, row in preview_df.iterrows():
        left_name = str(row.get("entity1_name", "")).strip()
        right_name = str(row.get("entity2_name", "")).strip()
        if left_name and left_name not in node_types:
            node_types[left_name] = str(row.get("entity1_type", "other"))
        if right_name and right_name not in node_types:
            node_types[right_name] = str(row.get("entity2_type", "other"))

    lines = [
        "digraph Relations {",
        "rankdir=LR;",
        'node [shape=ellipse, style=filled, fillcolor="#111c34",'
        ' color="#1f2d49", fontcolor="#e6edf7"];',
        'edge [color="#8bd3ff", fontcolor="#8ea0bf"];',
    ]

    for entity_name, entity_type in list(node_types.items())[:120]:
        label = _dot_escape(f"{entity_name}\\n({entity_type})")
        lines.append(f'"{_dot_escape(entity_name)}" [label="{label}"];')

    for _, row in preview_df.iterrows():
        left_name = str(row.get("entity1_name", "")).strip()
        right_name = str(row.get("entity2_name", "")).strip()
        if not left_name or not right_name:
            continue
        rel_text = short_text(str(row.get("relation_text", "")), 24)
        conf = float(row.get("confidence", 0.0) or 0.0)
        source_count = int(row.get("source_count", 0) or 0)
        edge_label = _dot_escape(f"{rel_text} | {conf:.2f} | {source_count}")
        penwidth = 1.0 + min(3.0, conf * 2.0) + min(2.0, source_count * 0.15)
        lines.append(
            f'"{_dot_escape(left_name)}" -> "{_dot_escape(right_name)}" '
            f'[label="{edge_label}", penwidth={penwidth:.2f}];'
        )

    lines.append("}")
    dot_graph = "\n".join(lines)

    try:
        st.graphviz_chart(dot_graph, use_container_width=True)
    except Exception as e:
        st.warning(f"å›¾è°±æ¸²æŸ“å¤±è´¥ï¼Œå·²é™çº§ä¸ºåˆ—è¡¨å±•ç¤º: {str(e)[:100]}")

    st.markdown("### å…³ç³»è¯¦æƒ…")
    for _, row in preview_df.head(20).iterrows():
        left_name = str(row.get("entity1_name", "N/A"))
        right_name = str(row.get("entity2_name", "N/A"))
        relation_text = str(row.get("relation_text", ""))
        confidence = float(row.get("confidence", 0.0) or 0.0)
        source_count = int(row.get("source_count", 0) or 0)
        with st.expander(f"{left_name} â†’ {right_name} ({confidence:.2f})"):
            st.write(f"å…³ç³»æè¿°: {relation_text}")
            st.write(f"æ¥æºæ•°: {source_count}")
            st.write(f"æœ€åå‡ºç°: {str(row.get('last_seen', 'N/A'))[:16]}")
            if bool(row.get("watchlist_related")):
                st.caption("è¯¥å…³ç³»ä¸æœ€è¿‘å“¨å…µå‘Šè­¦å®ä½“å­˜åœ¨äº¤é›†ã€‚")
            source_article_ids = row.get("source_article_ids", [])
            if not isinstance(source_article_ids, list):
                source_article_ids = parse_json_field(source_article_ids, list)
            if source_article_ids:
                st.caption(f"æ ·æœ¬æ–‡ç« ID: {', '.join([str(i) for i in source_article_ids[:8]])}")


def update_entities(supabase, cluster_id: int, entities: list, category: str):
    """æ›´æ–°å®ä½“è¡¨å’Œå®ä½“-èšç±»å…³è”è¡¨"""
    try:
        normalized_entities = normalize_entity_mentions(entities)
        for entity in normalized_entities:
            entity_name = entity["canonical_name"]
            entity_type = entity["entity_type"]
            metadata = merge_entity_metadata(
                existing_metadata={},
                entity=entity,
                model_name="qwen-plus",
                prompt_version="cluster_summary_v2",
            )

            # æ£€æŸ¥å®ä½“æ˜¯å¦å·²å­˜åœ¨
            existing = (
                supabase.table("entities")
                .select("id, mention_count_total, metadata")
                .eq("name", entity_name)
                .eq("entity_type", entity_type)
                .execute()
            )

            if existing.data:
                # æ›´æ–°ç°æœ‰å®ä½“
                entity_id = existing.data[0]["id"]
                new_count = existing.data[0]["mention_count_total"] + 1
                metadata = merge_entity_metadata(
                    existing_metadata=existing.data[0].get("metadata"),
                    entity=entity,
                    model_name="qwen-plus",
                    prompt_version="cluster_summary_v2",
                )

                supabase.table("entities").update(
                    {
                        "last_seen": datetime.now().isoformat(),
                        "mention_count_total": new_count,
                        "category": category,
                        "metadata": metadata,
                    }
                ).eq("id", entity_id).execute()
            else:
                # åˆ›å»ºæ–°å®ä½“
                result = (
                    supabase.table("entities")
                    .insert(
                        {
                            "name": entity_name,
                            "entity_type": entity_type,
                            "category": category,
                            "mention_count_total": 1,
                            "metadata": metadata,
                        }
                    )
                    .execute()
                )
                entity_id = result.data[0]["id"]

            # åˆ›å»ºæˆ–æ›´æ–°å®ä½“-èšç±»å…³è”
            try:
                supabase.table("entity_cluster_relations").upsert(
                    {
                        "entity_id": entity_id,
                        "cluster_id": cluster_id,
                        "mention_count": 1,
                    }
                ).execute()
            except Exception as e:
                logger.warning(f"å®ä½“å…³è”åˆ›å»ºå¤±è´¥ï¼ˆå¯èƒ½å·²å­˜åœ¨ï¼‰: {e}")

        logger.info(f"å®ä½“æ›´æ–°å®Œæˆ: {len(normalized_entities)} ä¸ªå®ä½“")
    except Exception as e:
        logger.error(f"æ›´æ–°å®ä½“å¤±è´¥: {e}")


# å®ä½“æ•°æ®è·å–å‡½æ•°
@st.cache_data(ttl=300)
def get_entities(
    _supabase, entity_type: str = None, category: str = None, limit: int = 50
) -> pd.DataFrame:
    """è·å–å®ä½“åˆ—è¡¨"""
    query = _supabase.table("entities").select("*")

    if entity_type and entity_type != "å…¨éƒ¨":
        query = query.eq("entity_type", entity_type)
    if category and category != "å…¨éƒ¨":
        query = query.eq("category", category)

    result = query.order("mention_count_total", desc=True).limit(limit).execute()

    if result.data:
        return pd.DataFrame(result.data)
    return pd.DataFrame()


@st.cache_data(ttl=300)
def get_entity_related_clusters(
    _supabase, entity_id: int, limit: int = 10
) -> pd.DataFrame:
    """è·å–å®ä½“å…³è”çš„èšç±»"""
    # è·å–å…³è”çš„èšç±»ID
    relations = (
        _supabase.table("entity_cluster_relations")
        .select("cluster_id")
        .eq("entity_id", entity_id)
        .limit(limit)
        .execute()
    )

    if not relations.data:
        return pd.DataFrame()

    cluster_ids = [r["cluster_id"] for r in relations.data]

    # è·å–èšç±»è¯¦æƒ…
    clusters = (
        _supabase.table("analysis_clusters")
        .select("id, primary_title, summary, category, created_at, article_count")
        .in_("id", cluster_ids)
        .execute()
    )

    if clusters.data:
        return pd.DataFrame(clusters.data)
    return pd.DataFrame()


@st.cache_data(ttl=300)
def get_trending_entities(_supabase, hours: int = 24, limit: int = 10) -> pd.DataFrame:
    """è·å–è¶‹åŠ¿ä¸Šå‡çš„å®ä½“"""
    cutoff = (datetime.now() - timedelta(hours=hours)).isoformat()

    result = (
        _supabase.table("entities")
        .select("*")
        .gte("last_seen", cutoff)
        .eq("trend_direction", "rising")
        .order("mention_count_24h", desc=True)
        .limit(limit)
        .execute()
    )

    if result.data:
        return pd.DataFrame(result.data)
    return pd.DataFrame()


# å®ä½“æ¡£æ¡ˆé¡µ
def render_entities(supabase):
    """æ¸²æŸ“å®ä½“æ¡£æ¡ˆé¡µ"""
    st.markdown('<div class="main-header">ğŸ“ å®ä½“æ¡£æ¡ˆ</div>', unsafe_allow_html=True)

    # è·å–ç­›é€‰æ¡ä»¶
    col1, col2 = st.columns(2)
    with col1:
        entity_type = st.selectbox(
            "å®ä½“ç±»å‹:",
            ["å…¨éƒ¨"] + ENTITY_TYPE_FILTER_OPTIONS,
        )
    with col2:
        category = st.selectbox(
            "æ‰€å±åˆ†ç±»:", ["å…¨éƒ¨", "military", "politics", "economy", "tech"]
        )

    # è·å–å®ä½“åˆ—è¡¨
    entities_df = get_entities(supabase, entity_type, category, limit=100)

    if entities_df.empty:
        st.info("æš‚æ— å®ä½“æ•°æ®")
        return

    # æ˜¾ç¤ºçƒ­é—¨å®ä½“
    st.markdown("### ğŸ”¥ çƒ­é—¨å®ä½“")

    top_entities = entities_df.head(10)
    cols = st.columns(5)
    for idx, (_, row) in enumerate(top_entities.iterrows()):
        with cols[idx % 5]:
            mention_count = row.get("mention_count_total", 0)
            st.metric(label=row.get("name", "N/A")[:15], value=f"{mention_count}æ¬¡")

    st.markdown("---")

    # å®ä½“åˆ—è¡¨
    st.markdown("### ğŸ“‹ å®ä½“åˆ—è¡¨")

    for idx, row in entities_df.iterrows():
        entity_id = row.get("id")
        entity_name = row.get("name", "N/A")
        entity_type_val = row.get("entity_type", "æœªçŸ¥")
        mention_count = row.get("mention_count_total", 0)
        last_seen = row.get("last_seen", "N/A")

        # è¶‹åŠ¿æŒ‡ç¤º
        trend = row.get("trend_direction", "stable")
        trend_icon = "ğŸ“ˆ" if trend == "rising" else "ğŸ“‰" if trend == "falling" else "â¡ï¸"

        with st.expander(
            f"{trend_icon} {entity_name} ({entity_type_val}) - æåŠ{mention_count}æ¬¡"
        ):
            col1, col2, col3 = st.columns(3)
            with col1:
                st.write(f"**ç±»å‹:** {entity_type_val}")
                st.write(f"**åˆ†ç±»:** {row.get('category', 'N/A')}")
            with col2:
                st.write(f"**24hæåŠ:** {row.get('mention_count_24h', 0)}")
                st.write(f"**7å¤©æåŠ:** {row.get('mention_count_7d', 0)}")
            with col3:
                st.write(f"**è¶‹åŠ¿:** {trend}")
                st.write(f"**æœ€åå‡ºç°:** {str(last_seen)[:10] if last_seen else 'N/A'}")

            # æ˜¾ç¤ºå…³è”èšç±»
            st.markdown("**ç›¸å…³çƒ­ç‚¹:**")
            related_clusters = get_entity_related_clusters(supabase, entity_id, limit=5)
            if not related_clusters.empty:
                for _, cluster in related_clusters.iterrows():
                    st.write(f"- {cluster.get('primary_title', 'N/A')}")
            else:
                st.write("æš‚æ— å…³è”çƒ­ç‚¹")

    # å®ä½“ç»Ÿè®¡å›¾è¡¨
    st.markdown("---")
    st.markdown("### ğŸ“Š å®ä½“ç»Ÿè®¡")

    col1, col2 = st.columns(2)
    with col1:
        # æŒ‰ç±»å‹ç»Ÿè®¡
        if not entities_df.empty and "entity_type" in entities_df.columns:
            type_counts = entities_df["entity_type"].value_counts()
            st.bar_chart(type_counts)
    with col2:
        # æŒ‰æåŠæ¬¡æ•°åˆ†å¸ƒ
        if not entities_df.empty and "mention_count_total" in entities_df.columns:
            mention_dist = pd.cut(
                entities_df["mention_count_total"],
                bins=[0, 1, 5, 10, 50, 1000],
                labels=["1æ¬¡", "2-5æ¬¡", "6-10æ¬¡", "11-50æ¬¡", "50+æ¬¡"],
            ).value_counts()
            st.bar_chart(mention_dist)


# æ•°æ®ç»Ÿè®¡é¡µ
def render_stats(supabase):
    """æ¸²æŸ“æ•°æ®ç»Ÿè®¡é¡µ"""
    st.markdown('<div class="main-header">ğŸ“ˆ æ•°æ®ç»Ÿè®¡</div>', unsafe_allow_html=True)

    # è·å–ç»Ÿè®¡æ•°æ®
    stats = get_stats(supabase)

    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("æ€»æ–‡ç« æ•°", f"{stats['articles_total']:,}")
    with col2:
        st.metric(
            "å·²åˆ†ææ–‡ç« ", f"{stats['articles_total'] - stats['articles_unanalyzed']:,}"
        )
    with col3:
        st.metric("å¾…åˆ†ææ–‡ç« ", stats["articles_unanalyzed"])

    st.markdown("---")

    # èšç±»è¶‹åŠ¿
    st.markdown("### ğŸ“Š èšç±»è¶‹åŠ¿ (æœ€è¿‘7å¤©)")

    # è·å–7å¤©æ•°æ®
    days_7 = (datetime.now() - timedelta(days=7)).isoformat()
    clusters_7d = (
        supabase.table("analysis_clusters")
        .select("created_at, category")
        .gte("created_at", days_7)
        .execute()
    )

    if clusters_7d.data:
        df = pd.DataFrame(clusters_7d.data)
        df["created_at"] = pd.to_datetime(df["created_at"]).dt.date

        # æŒ‰å¤©å’Œåˆ†ç±»ç»Ÿè®¡
        daily_counts = (
            df.groupby(["created_at", "category"]).size().unstack(fill_value=0)
        )
        st.line_chart(daily_counts)
    else:
        st.info("æš‚æ— æ•°æ®")

    # åˆ†ç±»å æ¯”
    st.markdown("### ğŸ¥§ åˆ†ç±»å æ¯”")

    all_clusters = supabase.table("analysis_clusters").select("category").execute()
    if all_clusters.data:
        df = pd.DataFrame(all_clusters.data)
        cat_counts = df["category"].value_counts()
        # ä½¿ç”¨æ¡å½¢å›¾ä»£æ›¿é¥¼å›¾
        st.bar_chart(cat_counts)


# ä¸»å‡½æ•°
def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ– Supabase
    supabase = init_supabase()

    if not supabase:
        st.error("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ã€‚è¯·æ£€æŸ¥é…ç½®ã€‚")
        return

    # æ¸²æŸ“ä¾§è¾¹æ 
    page, hours, category = render_sidebar()

    # æ ¹æ®é¡µé¢æ¸²æŸ“å†…å®¹
    if page == "ğŸ  æ¦‚è§ˆé¦–é¡µ":
        render_overview(supabase, hours, category)
    elif page == "ğŸ›°ï¸ å“¨å…µæ€åŠ¿":
        render_monitor(supabase, hours)
    elif page == "ğŸ•¸ï¸ å…³ç³»å›¾è°±":
        render_graph(supabase, hours)
    elif page == "ğŸ”¥ çƒ­ç‚¹è¯¦æƒ…":
        render_hotspots(supabase, hours, category)
    elif page == "ğŸ“¡ ä¿¡å·ä¸­å¿ƒ":
        render_signals(supabase, hours)
    elif page == "ğŸ“ å®ä½“æ¡£æ¡ˆ":
        render_entities(supabase)
    elif page == "ğŸ“ˆ æ•°æ®ç»Ÿè®¡":
        render_stats(supabase)


if __name__ == "__main__":
    main()
