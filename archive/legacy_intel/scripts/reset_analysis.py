#!/usr/bin/env python3
"""
é‡ç½®æ–‡ç« åˆ†æçŠ¶æ€
ç”¨äºé‡æ–°åˆ†æä¹‹å‰å¤±è´¥çš„æ–‡ç« 
"""

import os
import sys
from datetime import datetime, timedelta
from typing import List

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from supabase import create_client


def _fetch_all_rows(
    supabase, table_name: str, columns: str, apply_filters=None, page_size: int = 1000
) -> List[dict]:
    """åˆ†é¡µè¯»å–å…¨éƒ¨è¡Œï¼Œé¿å… PostgREST é»˜è®¤åˆ†é¡µå¯¼è‡´é—æ¼"""
    all_rows: List[dict] = []
    offset = 0

    while True:
        query = (
            supabase.table(table_name)
            .select(columns)
            .range(offset, offset + page_size - 1)
        )
        if apply_filters:
            query = apply_filters(query)
        result = query.execute()
        rows = result.data or []
        all_rows.extend(rows)
        if len(rows) < page_size:
            break
        offset += page_size

    return all_rows


def _reset_articles_by_ids(supabase, article_ids: List[int], batch_size: int = 200) -> int:
    """æŒ‰æ‰¹æ¬¡é‡ç½®æ–‡ç«  analyzed_at å­—æ®µï¼Œé¿å…å•æ¡æ›´æ–°è¿‡æ…¢"""
    if not article_ids:
        return 0

    reset_count = 0
    for i in range(0, len(article_ids), batch_size):
        batch_ids = article_ids[i : i + batch_size]
        supabase.table("articles").update({"analyzed_at": None}).in_("id", batch_ids).execute()
        reset_count += len(batch_ids)

    return reset_count


def _cleanup_entities_for_clusters(supabase, cluster_ids: List[int]) -> int:
    """æ¸…ç†æŒ‡å®šèšç±»å…³è”çš„å®ä½“ï¼Œå¹¶åˆ é™¤å¤±å»å…³è”çš„å­¤ç«‹å®ä½“"""
    if not cluster_ids:
        return 0

    candidate_entity_ids = set()
    batch_size = 200
    for i in range(0, len(cluster_ids), batch_size):
        batch_cluster_ids = cluster_ids[i : i + batch_size]

        relations = (
            supabase.table("entity_cluster_relations")
            .select("entity_id")
            .in_("cluster_id", batch_cluster_ids)
            .execute()
        )
        for row in relations.data or []:
            candidate_entity_ids.add(row["entity_id"])

        # å…ˆåˆ èšç±»å…³è”
        supabase.table("entity_cluster_relations").delete().in_(
            "cluster_id", batch_cluster_ids
        ).execute()

    deleted_entities = 0
    for entity_id in candidate_entity_ids:
        left_relations = (
            supabase.table("entity_cluster_relations")
            .select("id")
            .eq("entity_id", entity_id)
            .limit(1)
            .execute()
        )
        if not left_relations.data:
            supabase.table("entities").delete().eq("id", entity_id).execute()
            deleted_entities += 1

    return deleted_entities


def reset_analysis_status(hours: int = 24, reset_all: bool = False):
    """
    é‡ç½®æ–‡ç« åˆ†æçŠ¶æ€

    Args:
        hours: é‡ç½®æœ€è¿‘å¤šå°‘å°æ—¶å†…çš„æ–‡ç« 
        reset_all: æ˜¯å¦é‡ç½®æ‰€æœ‰å·²åˆ†æçš„æ–‡ç« 
    """
    supabase_url = os.getenv("SUPABASE_URL")
    supabase_key = os.getenv("SUPABASE_KEY")

    if not supabase_url or not supabase_key:
        print("âŒ ç¼ºå°‘ Supabase é…ç½®")
        return

    supabase = create_client(supabase_url, supabase_key)

    if reset_all:
        # é‡ç½®æ‰€æœ‰å·²åˆ†æçš„æ–‡ç« 
        print("âš ï¸  é‡ç½®æ‰€æœ‰å·²åˆ†æçš„æ–‡ç« ...")
        analyzed_count_result = (
            supabase.table("articles")
            .select("id", count="exact")
            .not_.is_("analyzed_at", "null")
            .limit(1)
            .execute()
        )
        reset_count = analyzed_count_result.count or 0
        supabase.table("articles").update({"analyzed_at": None}).not_.is_(
            "analyzed_at", "null"
        ).execute()
        print(f"âœ… å·²é‡ç½® {reset_count} ç¯‡æ–‡ç« ")

        # åˆ é™¤æ‰€æœ‰åˆ†æèšç±»
        print("âš ï¸  åˆ é™¤æ‰€æœ‰åˆ†æèšç±»...")
        supabase.table("analysis_clusters").delete().neq("id", 0).execute()
        print("âœ… å·²åˆ é™¤æ‰€æœ‰èšç±»")

        # åˆ é™¤æ‰€æœ‰ä¿¡å·
        print("âš ï¸  åˆ é™¤æ‰€æœ‰ä¿¡å·...")
        supabase.table("analysis_signals").delete().neq("id", 0).execute()
        print("âœ… å·²åˆ é™¤æ‰€æœ‰ä¿¡å·")

        # åˆ é™¤æ‰€æœ‰å®ä½“å…³è”
        print("âš ï¸  åˆ é™¤æ‰€æœ‰å®ä½“å…³è”...")
        supabase.table("entity_cluster_relations").delete().neq("id", 0).execute()
        print("âœ… å·²åˆ é™¤æ‰€æœ‰å®ä½“å…³è”")

        # åˆ é™¤æ‰€æœ‰å®ä½“æ¡£æ¡ˆ
        print("âš ï¸  åˆ é™¤æ‰€æœ‰å®ä½“æ¡£æ¡ˆ...")
        supabase.table("entities").delete().neq("id", 0).execute()
        print("âœ… å·²åˆ é™¤æ‰€æœ‰å®ä½“æ¡£æ¡ˆ")

    else:
        # åªé‡ç½®æœ€è¿‘ N å°æ—¶çš„æ–‡ç« 
        cutoff_time = (datetime.now() - timedelta(hours=hours)).isoformat()

        print(f"ğŸ“ é‡ç½®æœ€è¿‘ {hours} å°æ—¶å†…åˆ†æçš„æ–‡ç« ...")

        # ç»Ÿè®¡è¿™äº›æ–‡ç« 
        result = (
            supabase.table("articles")
            .select("id", count="exact")
            .gte("analyzed_at", cutoff_time)
            .limit(1)
            .execute()
        )

        reset_count = result.count or 0

        if not reset_count:
            print("âœ… æ²¡æœ‰æ‰¾åˆ°éœ€è¦é‡ç½®çš„æ–‡ç« ")
            return

        print(f"ğŸ“Š æ‰¾åˆ° {reset_count} ç¯‡æ–‡ç« éœ€è¦é‡ç½®")

        # é‡ç½®æ–‡ç« çŠ¶æ€
        supabase.table("articles").update({"analyzed_at": None}).gte(
            "analyzed_at", cutoff_time
        ).execute()
        print(f"âœ… å·²é‡ç½® {reset_count} ç¯‡æ–‡ç« çš„åˆ†æçŠ¶æ€")

        # åˆ é™¤ç›¸å…³çš„èšç±»
        print("ğŸ—‘ï¸  åˆ é™¤ç›¸å…³çš„èšç±»...")
        clusters = _fetch_all_rows(
            supabase,
            "analysis_clusters",
            "id",
            apply_filters=lambda q: q.gte("created_at", cutoff_time),
        )
        cluster_ids = [c["id"] for c in clusters]

        # åˆ é™¤ç›¸å…³ä¿¡å·
        supabase.table("analysis_signals").delete().gte("created_at", cutoff_time).execute()
        print("âœ… å·²åˆ é™¤æ—¶é—´çª—å£å†…çš„ä¿¡å·")

        # åˆ é™¤å…³è”å®ä½“ï¼ˆä»…æ¸…ç†æ— å…³è”çš„å­¤ç«‹å®ä½“ï¼‰
        deleted_entities = _cleanup_entities_for_clusters(supabase, cluster_ids)
        if cluster_ids:
            print(f"âœ… å·²æ¸…ç† {len(cluster_ids)} ä¸ªèšç±»å…³è”ï¼Œå¹¶åˆ é™¤ {deleted_entities} ä¸ªå­¤ç«‹å®ä½“")

        for cluster_id in cluster_ids:
            # åˆ é™¤å…³è”
            supabase.table("article_analyses").delete().eq(
                "cluster_id", cluster_id
            ).execute()
            # åˆ é™¤èšç±»
            supabase.table("analysis_clusters").delete().eq("id", cluster_id).execute()

        print(f"âœ… å·²åˆ é™¤ {len(cluster_ids)} ä¸ªèšç±»")


def reset_shallow_analysis():
    """
    åªé‡ç½®æµ…å±‚åˆ†æçš„æ–‡ç« ï¼ˆè®©çƒ­ç‚¹è¿›è¡Œæ·±åº¦åˆ†æï¼‰
    """
    supabase_url = os.getenv("SUPABASE_URL")
    supabase_key = os.getenv("SUPABASE_KEY")

    if not supabase_url or not supabase_key:
        print("âŒ ç¼ºå°‘ Supabase é…ç½®")
        return

    supabase = create_client(supabase_url, supabase_key)

    print("ğŸ“ æŸ¥æ‰¾æµ…å±‚åˆ†æçš„èšç±»...")

    # è·å–æµ…å±‚åˆ†æçš„èšç±»
    clusters = _fetch_all_rows(
        supabase,
        "analysis_clusters",
        "id",
        apply_filters=lambda q: q.eq("analysis_depth", "shallow"),
    )

    if not clusters:
        print("âœ… æ²¡æœ‰æµ…å±‚åˆ†æçš„èšç±»")
        return

    print(f"ğŸ“Š æ‰¾åˆ° {len(clusters)} ä¸ªæµ…å±‚åˆ†æèšç±»")

    # è·å–å…³è”çš„æ–‡ç« ID
    cluster_ids = [c["id"] for c in clusters]
    article_ids = set()
    batch_size = 200
    for i in range(0, len(cluster_ids), batch_size):
        batch_cluster_ids = cluster_ids[i : i + batch_size]
        relations = (
            supabase.table("article_analyses")
            .select("article_id")
            .in_("cluster_id", batch_cluster_ids)
            .execute()
        )
        for row in relations.data or []:
            article_ids.add(row["article_id"])

    print(f"ğŸ“Š æ¶‰åŠ {len(article_ids)} ç¯‡æ–‡ç« ")

    # é‡ç½®æ–‡ç« çŠ¶æ€
    reset_count = _reset_articles_by_ids(supabase, list(article_ids))
    print(f"âœ… å·²é‡ç½® {reset_count} ç¯‡æ–‡ç« ")

    # æ¸…ç†å®ä½“å…³è”å¹¶åˆ é™¤å­¤ç«‹å®ä½“
    deleted_entities = _cleanup_entities_for_clusters(supabase, cluster_ids)
    print(f"âœ… å·²åˆ é™¤ {deleted_entities} ä¸ªå­¤ç«‹å®ä½“")

    # åˆ é™¤æµ…å±‚èšç±»
    for cluster_id in cluster_ids:
        supabase.table("article_analyses").delete().eq(
            "cluster_id", cluster_id
        ).execute()
        supabase.table("analysis_clusters").delete().eq("id", cluster_id).execute()

    print(f"âœ… å·²åˆ é™¤ {len(cluster_ids)} ä¸ªæµ…å±‚èšç±»")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="é‡ç½®æ–‡ç« åˆ†æçŠ¶æ€")
    parser.add_argument(
        "--hours", type=int, default=24, help="é‡ç½®æœ€è¿‘å¤šå°‘å°æ—¶å†…çš„æ–‡ç«  (é»˜è®¤: 24)"
    )
    parser.add_argument("--all", action="store_true", help="é‡ç½®æ‰€æœ‰å·²åˆ†æçš„æ–‡ç« ")
    parser.add_argument(
        "--shallow-only", action="store_true", help="åªé‡ç½®æµ…å±‚åˆ†æçš„æ–‡ç« "
    )

    args = parser.parse_args()

    print("=" * 60)
    print("ğŸ”§ é‡ç½®æ–‡ç« åˆ†æçŠ¶æ€")
    print("=" * 60)

    if args.shallow_only:
        reset_shallow_analysis()
    else:
        reset_analysis_status(hours=args.hours, reset_all=args.all)

    print("=" * 60)
    print("âœ… é‡ç½®å®Œæˆï¼ç°åœ¨å¯ä»¥é‡æ–°è¿è¡Œ analyzer.py äº†")
    print("=" * 60)
