#!/usr/bin/env python3
"""
å®Œæ•´ç³»ç»Ÿæµ‹è¯• - ä½¿ç”¨ä»£ç†
"""

import os

os.environ["https_proxy"] = "http://127.0.0.1:6152"
os.environ["http_proxy"] = "http://127.0.0.1:6152"
os.environ["all_proxy"] = "socks5://127.0.0.1:6153"

import asyncio
import aiohttp
import feedparser
from supabase import create_client

SUPABASE_URL = "https://lwigqxyfxevldfjdeokp.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Imx3aWdxeHlmeGV2bGRmamRlb2twIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc3MTE3MzYxMiwiZXhwIjoyMDg2NzQ5NjEyfQ.-JCEODgYe83EugQeTxLHsxBXikXbz_btei9-qsUxb1M"
WORKER_URL = "https://content-extractor.linkwild0101.workers.dev"

print("=" * 60)
print("ğŸ§ª RSS çˆ¬è™«ç³»ç»Ÿæµ‹è¯•")
print("=" * 60)

# æµ‹è¯•1: æ•°æ®åº“è¿æ¥
print("\n1ï¸âƒ£  æµ‹è¯•æ•°æ®åº“è¿æ¥...")
try:
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    result = supabase.table("rss_sources").select("count", count="exact").execute()
    count = result.count
    print(f"   âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
    print(f"   ğŸ“Š RSSæºæ•°é‡: {count}")
except Exception as e:
    print(f"   âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
    exit(1)

# æµ‹è¯•2: è·å–æµ‹è¯•æº
print("\n2ï¸âƒ£  è·å–æµ‹è¯•æº...")
try:
    sources = supabase.table("rss_sources").select("*").limit(3).execute().data
    print(f"   âœ… è·å–åˆ° {len(sources)} ä¸ªæµ‹è¯•æº")
    for s in sources:
        print(f"      - {s['name']} ({s['category']})")
except Exception as e:
    print(f"   âŒ è·å–æºå¤±è´¥: {e}")
    exit(1)

# æµ‹è¯•3: RSSæŠ“å–
print("\n3ï¸âƒ£  æµ‹è¯•RSSæŠ“å–...")


async def test_rss():
    timeout = aiohttp.ClientTimeout(total=30)
    connector = aiohttp.TCPConnector(ssl=False)

    async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
        success_count = 0
        for source in sources:
            try:
                print(f"\n   ğŸ“° {source['name'][:40]}...")
                async with session.get(
                    source["rss_url"],
                    headers={"User-Agent": "Mozilla/5.0 (compatible; RSSCrawler/1.0)"},
                ) as resp:
                    if resp.status == 200:
                        content = await resp.text()
                        feed = feedparser.parse(content)
                        if feed.entries:
                            print(f"      âœ… æˆåŠŸ! è·å– {len(feed.entries)} ç¯‡æ–‡ç« ")
                            print(
                                f"      ğŸ“ æœ€æ–°: {feed.entries[0].get('title', 'N/A')[:50]}..."
                            )
                            success_count += 1
                        else:
                            print(f"      âš ï¸  RSSè§£ææˆåŠŸä½†æ— æ–‡ç« ")
                    else:
                        print(f"      âš ï¸  HTTP {resp.status}")
            except Exception as e:
                print(f"      âŒ é”™è¯¯: {str(e)[:50]}")

        return success_count


success = asyncio.run(test_rss())
print(f"\n   ğŸ“Š RSSæŠ“å–æˆåŠŸç‡: {success}/{len(sources)}")

# æµ‹è¯•4: å†…å®¹æå–ï¼ˆWorkerï¼‰
print("\n4ï¸âƒ£  æµ‹è¯•å†…å®¹æå–æœåŠ¡...")
try:
    import requests

    proxies = {"http": "http://127.0.0.1:6152", "https": "http://127.0.0.1:6152"}
    resp = requests.post(
        f"{WORKER_URL}/extract",
        json={"url": "https://www.reuters.com/business/"},
        headers={"Content-Type": "application/json"},
        proxies=proxies,
        timeout=30,
    )
    if resp.status_code == 200:
        data = resp.json()
        if data.get("success"):
            print(f"   âœ… Workerè¿è¡Œæ­£å¸¸")
            print(f"   ğŸ“ æå–æ ‡é¢˜: {data.get('title', 'N/A')[:40]}...")
        else:
            print(f"   âš ï¸  Workerè¿”å›é”™è¯¯: {data.get('error')}")
    else:
        print(f"   âš ï¸  HTTP {resp.status_code}")
except Exception as e:
    print(f"   âš ï¸  Workeræµ‹è¯•è·³è¿‡: {str(e)[:50]}")

# æµ‹è¯•5: æ•°æ®å†™å…¥
print("\n5ï¸âƒ£  æµ‹è¯•æ•°æ®å†™å…¥...")
try:
    test_article = {
        "title": "Test Article - " + str(asyncio.get_event_loop().time()),
        "content": "This is a test article content.",
        "url": f"https://test.example.com/{asyncio.get_event_loop().time()}",
        "source_id": sources[0]["id"] if sources else 1,
        "category": "test",
        "extraction_method": "test",
    }
    result = supabase.table("articles").insert(test_article).execute()
    if result.data:
        print(f"   âœ… æ•°æ®å†™å…¥æˆåŠŸ")
        # æ¸…ç†æµ‹è¯•æ•°æ®
        supabase.table("articles").delete().eq("url", test_article["url"]).execute()
        print(f"   âœ… æµ‹è¯•æ•°æ®å·²æ¸…ç†")
    else:
        print(f"   âŒ æ•°æ®å†™å…¥å¤±è´¥")
except Exception as e:
    print(f"   âŒ æ•°æ®å†™å…¥é”™è¯¯: {e}")

print("\n" + "=" * 60)
print("âœ… ç³»ç»Ÿæµ‹è¯•å®Œæˆ!")
print("=" * 60)
print("\nğŸ“‹ æ€»ç»“:")
print("   â€¢ æ•°æ®åº“è¿æ¥: âœ… æ­£å¸¸")
print("   â€¢ RSSæºæ•°é‡: âœ… {} ä¸ª".format(count))
print("   â€¢ RSSæŠ“å–: âœ… {} ä¸ªæºæˆåŠŸ".format(success))
print("   â€¢ å†…å®¹æå–: æµ‹è¯•ä¸­ï¼ˆå¯èƒ½å—ç½‘ç»œå½±å“ï¼‰")
print("   â€¢ æ•°æ®å†™å…¥: âœ… æ­£å¸¸")
print("\nğŸš€ ç³»ç»Ÿå·²å°±ç»ªï¼Œå¯ä»¥è¿è¡Œå®Œæ•´çˆ¬è™«!")
