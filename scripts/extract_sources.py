#!/usr/bin/env python3
"""
æå–RSSæºæ•°æ®ä»Markdownæ–‡ä»¶
æ”¯æŒå¤šç§æ ¼å¼ï¼šus_military_news_sources_214.md, us_economy_finance_sources_151.md, us_politics_news_sources_100plus.txt
"""

import re
import json
import os
from pathlib import Path


def parse_markdown_table(content, category):
    """è§£æMarkdownè¡¨æ ¼æ ¼å¼"""
    sources = []
    lines = content.split("\n")

    # æ‰¾åˆ°è¡¨æ ¼å¼€å§‹ï¼ˆåŒ…å« | Name | çš„è¡Œï¼‰
    table_start = None
    for i, line in enumerate(lines):
        if "| Name " in line and "| RSS URL " in line:
            table_start = i + 2  # è·³è¿‡è¡¨å¤´å’Œåˆ†éš”ç¬¦
            break

    if table_start is None:
        return sources

    # è§£æè¡¨æ ¼è¡Œ
    for line in lines[table_start:]:
        line = line.strip()
        if not line or not line.startswith("|"):
            continue
        if line.startswith("|---"):
            continue

        # åˆ†å‰²å•å…ƒæ ¼
        cells = [cell.strip() for cell in line.split("|")[1:-1]]
        if len(cells) < 3:
            continue

        # æå–å­—æ®µ
        name = cells[0] if len(cells) > 0 else ""
        listing_url = cells[1] if len(cells) > 1 else ""
        rss_url = cells[2] if len(cells) > 2 else ""
        description = cells[3] if len(cells) > 3 else ""
        anti_scraping = cells[4] if len(cells) > 4 else "None"

        # æ¸…ç†æ•°æ®
        name = name.strip()
        rss_url = rss_url.strip()

        # è·³è¿‡æ— æ•ˆæ•°æ®
        if not name or not rss_url or rss_url == "RSS URL":
            continue
        if not rss_url.startswith("http"):
            continue

        sources.append(
            {
                "name": name,
                "listing_url": listing_url.strip() if listing_url else "",
                "rss_url": rss_url,
                "description": description.strip() if description else "",
                "category": category,
                "anti_scraping": anti_scraping.strip() if anti_scraping else "None",
                "status": "active",
            }
        )

    return sources


def extract_sources():
    """ä¸»å‡½æ•°ï¼šä»æ‰€æœ‰æºæ–‡ä»¶æå–RSSæº"""

    base_dir = Path("/Users/nobody1/Documents/US_newslist")
    all_sources = []
    stats = {"military": 0, "economy": 0, "politics": 0}

    # æ–‡ä»¶æ˜ å°„
    files_to_parse = [
        ("us_military_news_sources_214.md", "military"),
        ("us_economy_finance_sources_151.md", "economy"),
        ("us_politics_news_sources_100plus.txt", "politics"),
    ]

    for filename, category in files_to_parse:
        filepath = base_dir / filename

        if not filepath.exists():
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
            continue

        print(f"ğŸ“„ æ­£åœ¨è§£æ: {filename} ({category})")

        try:
            with open(filepath, "r", encoding="utf-8") as f:
                content = f.read()

            sources = parse_markdown_table(content, category)
            stats[category] = len(sources)
            all_sources.extend(sources)

            print(f"   âœ… æå–äº† {len(sources)} ä¸ªæº")

        except Exception as e:
            print(f"   âŒ é”™è¯¯: {e}")

    # å»é‡ï¼ˆåŸºäºrss_urlï¼‰
    seen_urls = set()
    unique_sources = []
    duplicates = 0

    for source in all_sources:
        url = source["rss_url"]
        if url not in seen_urls:
            seen_urls.add(url)
            unique_sources.append(source)
        else:
            duplicates += 1

    # æ·»åŠ ID
    for i, source in enumerate(unique_sources, 1):
        source["id"] = i

    # ä¿å­˜JSON
    output_file = base_dir / "data" / "sources.json"
    output_file.parent.mkdir(exist_ok=True)

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(unique_sources, f, indent=2, ensure_ascii=False)

    # æ‰“å°ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("ğŸ“Š æå–ç»Ÿè®¡")
    print("=" * 60)
    print(f"å†›äº‹ (military):    {stats['military']:3d} ä¸ª")
    print(f"ç»æµ (economy):     {stats['economy']:3d} ä¸ª")
    print(f"æ”¿æ²» (politics):    {stats['politics']:3d} ä¸ª")
    print(f"-" * 60)
    print(f"æ€»è®¡:               {len(all_sources):3d} ä¸ª")
    print(f"å»é‡å:             {len(unique_sources):3d} ä¸ª")
    print(f"é‡å¤æ•°:             {duplicates:3d} ä¸ª")
    print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")

    # éªŒè¯JSON
    print(f"\nğŸ” éªŒè¯JSONæ ¼å¼...")
    with open(output_file, "r", encoding="utf-8") as f:
        data = json.load(f)
    print(f"âœ… JSONæ ¼å¼æ­£ç¡®ï¼Œå…± {len(data)} æ¡è®°å½•")

    # æ˜¾ç¤ºå‰3ä¸ªç¤ºä¾‹
    print(f"\nğŸ“‹ å‰3ä¸ªæºç¤ºä¾‹:")
    for source in data[:3]:
        print(f"   [{source['category']}] {source['name']}")
        print(f"       RSS: {source['rss_url'][:60]}...")

    return unique_sources


if __name__ == "__main__":
    extract_sources()
