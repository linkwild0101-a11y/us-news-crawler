#!/usr/bin/env python3
"""
RSSçˆ¬è™«æ ¸å¿ƒ - å¼‚æ­¥æŠ“å–RSSå¹¶æå–å†…å®¹
"""

import asyncio
import aiohttp
import feedparser
import json
import os
import hashlib
import re
from datetime import datetime
from typing import List, Dict, Optional
from supabase import create_client
from urllib.parse import quote, urlparse, urlunparse

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
WORKER_URL = os.getenv("WORKER_URL")
RAILWAY_URL = os.getenv("RAILWAY_URL")


class RSSCrawler:
    def __init__(self):
        self.supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        self.session: Optional[aiohttp.ClientSession] = None
        self.semaphore = asyncio.Semaphore(20)  # é™åˆ¶å¹¶å‘æ•°
        self.stats = {
            "sources_processed": 0,
            "articles_fetched": 0,
            "articles_new": 0,
            "articles_deduped": 0,
            "errors": 0,
        }

    def _log(self, message: str):
        """ç»Ÿä¸€æ—¥å¿—è¾“å‡ºï¼Œç¡®ä¿CIç¯å¢ƒå®æ—¶åˆ·æ–°"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] {message}", flush=True)

    def _normalize_article_url(self, url: str) -> str:
        """ä¿®å¤éƒ¨åˆ†RSSæºè¿”å›çš„å¼‚å¸¸é“¾æ¥ï¼ˆå¦‚ .twtwï¼‰ã€‚"""
        if not url:
            return ""

        normalized = str(url).strip()
        if not normalized:
            return ""

        if normalized.startswith("//"):
            normalized = f"https:{normalized}"

        parsed = urlparse(normalized)
        if not parsed.scheme:
            if normalized.startswith("/"):
                return normalized
            parsed = urlparse(f"https://{normalized.lstrip('/')}")

        host = parsed.hostname or ""
        if not host:
            return normalized

        # ä¾‹å¦‚ www.ydn.com.twtw -> www.ydn.com.tw
        fixed_host = re.sub(r"\.([a-z]{2})\1$", r".\1", host)
        if fixed_host == host:
            return normalized

        netloc = fixed_host
        if parsed.port:
            netloc = f"{netloc}:{parsed.port}"

        fixed_url = urlunparse(
            (
                parsed.scheme or "https",
                netloc,
                parsed.path,
                parsed.params,
                parsed.query,
                parsed.fragment,
            )
        )
        self._log(f"ğŸ”§ ä¿®æ­£å¼‚å¸¸é“¾æ¥: {normalized} -> {fixed_url}")
        return fixed_url

    async def __aenter__(self):
        timeout = aiohttp.ClientTimeout(total=30)
        self.session = aiohttp.ClientSession(timeout=timeout)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def fetch_rss(self, source: Dict) -> Optional[Dict]:
        """æŠ“å–å•ä¸ªRSSæº"""
        async with self.semaphore:
            try:
                rss_url = source["rss_url"]
                anti_scraping = source.get("anti_scraping", "None")

                # 1) å…ˆå°è¯•ç›´æ¥æŠ“å–
                content = await self._fetch_rss_direct(rss_url)

                # 2) ç›´æ¥å¤±è´¥åï¼Œæ ¹æ®ç­–ç•¥èµ°ä»£ç†é‡è¯•
                if not content:
                    if anti_scraping == "railway":
                        content = await self._fetch_rss_via_railway(rss_url)
                        if not content:
                            content = await self._fetch_rss_via_worker(rss_url)
                    elif anti_scraping in ["Cloudflare", "Paywall", "Partial Paywall"]:
                        content = await self._fetch_rss_via_worker(rss_url)
                        if not content:
                            content = await self._fetch_rss_via_railway(rss_url)

                if not content:
                    raise Exception("RSSå†…å®¹ä¸ºç©º")

                feed = feedparser.parse(content)
                if not feed.entries:
                    raise Exception("RSSæ— æœ‰æ•ˆæ¡ç›®")

                fetch_method = "direct"
                if anti_scraping == "railway" and WORKER_URL and RAILWAY_URL:
                    fetch_method = "railway/worker-fallback"
                elif anti_scraping in ["Cloudflare", "Paywall", "Partial Paywall"]:
                    fetch_method = "worker/railway-fallback"

                self._log(
                    f"âœ… RSSæŠ“å–æˆåŠŸ {source['name']} | æ¡ç›®: {len(feed.entries[:10])} | "
                    f"ç­–ç•¥: {fetch_method}"
                )

                return {
                    "source_id": source["id"],
                    "category": source["category"],
                    "anti_scraping": anti_scraping,
                    "entries": feed.entries[:10],  # åªå–å‰10æ¡
                }
            except Exception as e:
                self._log(f"âš ï¸  RSSæŠ“å–å¤±è´¥ {source['name']}: {e}")
                self.stats["errors"] += 1
                return None

    async def _fetch_rss_direct(self, rss_url: str) -> Optional[str]:
        """ç›´æ¥æŠ“å–RSSå†…å®¹"""
        try:
            async with self.session.get(
                rss_url,
                headers={"User-Agent": "Mozilla/5.0 (compatible; RSSCrawler/1.0)"},
            ) as resp:
                if resp.status != 200:
                    return None
                return await resp.text()
        except Exception:
            return None

    async def _fetch_rss_via_worker(self, rss_url: str) -> Optional[str]:
        """é€šè¿‡Cloudflare WorkeræŠ“å–RSSå†…å®¹"""
        if not WORKER_URL:
            return None
        try:
            async with self.session.post(
                f"{WORKER_URL}/extract",
                json={"url": rss_url, "raw": True},
                headers={"Content-Type": "application/json"},
            ) as resp:
                if resp.status != 200:
                    return None
                data = await resp.json()
                if not data.get("success"):
                    return None
                return data.get("content", "")
        except Exception:
            return None

    async def _fetch_rss_via_railway(self, rss_url: str) -> Optional[str]:
        """é€šè¿‡Railwayä»£ç†æŠ“å–RSSå†…å®¹"""
        if not RAILWAY_URL:
            return None
        try:
            encoded_url = quote(rss_url, safe="")
            async with self.session.get(
                f"{RAILWAY_URL}/rss?url={encoded_url}",
                headers={"Accept": "application/xml"},
            ) as resp:
                if resp.status != 200:
                    return None
                return await resp.text()
        except Exception:
            return None

    async def extract_content(self, url: str, anti_scraping: str) -> Optional[Dict]:
        """æ··åˆå†…å®¹æå–"""
        # è·³è¿‡ Twitter/X é“¾æ¥ï¼ˆå·²çŸ¥ä¼šæœ‰ header è¿‡é•¿é—®é¢˜ï¼‰
        if "twitter.com" in url or "x.com" in url:
            print(f"  â­ï¸  è·³è¿‡ Twitter/X é“¾æ¥")
            return None

        try:
            if anti_scraping in ["Cloudflare", "Paywall"] and WORKER_URL:
                # ä½¿ç”¨Cloudflare Worker
                async with self.session.post(
                    f"{WORKER_URL}/extract",
                    json={"url": url},
                    headers={"Content-Type": "application/json"},
                ) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        if data.get("success"):
                            data["extraction_method"] = "cloudflare"
                            return data

            # æœ¬åœ°æå–ï¼ˆç®€åŒ–ç‰ˆï¼‰
            async with self.session.get(
                url, headers={"User-Agent": "Mozilla/5.0 (compatible; RSSCrawler/1.0)"}
            ) as resp:
                if resp.status == 200:
                    html = await resp.text()
                    # ç®€å•æå–æ ‡é¢˜å’Œæ­£æ–‡
                    title = self._extract_title(html)
                    content = self._extract_content_simple(html)
                    return {
                        "title": title,
                        "content": content,
                        "extraction_method": "local",
                    }
        except Exception as e:
            print(f"  âš ï¸  å†…å®¹æå–å¤±è´¥: {e}")

        return None

    def _extract_title(self, html: str) -> str:
        """ä»HTMLä¸­æå–æ ‡é¢˜"""
        import re

        match = re.search(r"<title[^>]*>([^<]*)</title>", html, re.I)
        return match.group(1).strip() if match else ""

    def _extract_content_simple(self, html: str) -> str:
        """ç®€å•æå–æ­£æ–‡"""
        import re

        # ç§»é™¤scriptå’Œstyle
        html = re.sub(r"<script[^>]*>[\s\S]*?</script>", "", html, flags=re.I)
        html = re.sub(r"<style[^>]*>[\s\S]*?</style>", "", html, flags=re.I)
        # æå–æ–‡æœ¬
        text = re.sub(r"<[^>]+>", " ", html)
        # æ¸…ç†
        text = " ".join(text.split())
        return text[:5000]  # é™åˆ¶é•¿åº¦

    def compute_simhash(self, text: str) -> str:
        """è®¡ç®—SimHash"""
        try:
            from simhash import Simhash

            if not text:
                return "0"
            return str(Simhash(text[:1000]))
        except ImportError:
            # å¦‚æœæ²¡æœ‰simhashåº“ï¼Œä½¿ç”¨MD5ä½œä¸ºfallback
            return hashlib.md5(text[:1000].encode()).hexdigest()

    async def check_duplicate(self, simhash: str, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦é‡å¤"""
        try:
            # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
            result = (
                self.supabase.table("articles").select("id").eq("url", url).execute()
            )
            if result.data:
                return True

            # SimHashæ£€æŸ¥ï¼ˆæœ€è¿‘7å¤©ï¼‰
            from datetime import timedelta

            cutoff = (datetime.now() - timedelta(days=7)).isoformat()

            result = (
                self.supabase.table("articles")
                .select("id, simhash")
                .gte("fetched_at", cutoff)
                .execute()
            )

            for article in result.data:
                if (
                    article.get("simhash")
                    and self._hamming_distance(simhash, article["simhash"]) <= 3
                ):
                    return True

            return False
        except Exception as e:
            self._log(f"âš ï¸  å»é‡æ£€æŸ¥å¤±è´¥: {e}")
            return False

    def _hamming_distance(self, hash1: str, hash2: str) -> int:
        """è®¡ç®—æ±‰æ˜è·ç¦»"""
        try:
            h1 = int(hash1)
            h2 = int(hash2)
            x = h1 ^ h2
            distance = 0
            while x:
                distance += 1
                x &= x - 1
            return distance
        except:
            return 100  # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè¿”å›å¤§æ•°è¡¨ç¤ºä¸ç›¸ä¼¼

    async def save_article(self, article: Dict) -> bool:
        """ä¿å­˜æ–‡ç« åˆ°æ•°æ®åº“"""
        try:
            result = (
                self.supabase.table("articles")
                .insert(
                    {
                        "title": article["title"][:500],
                        "content": article.get("content", "")[:10000],
                        "url": article["url"],
                        "source_id": article["source_id"],
                        "published_at": article.get("published_at"),
                        "fetched_at": datetime.now().isoformat(),
                        "simhash": article.get("simhash"),
                        "category": article.get("category"),
                        "author": article.get("author", "")[:255],
                        "extraction_method": article.get("extraction_method", "local"),
                    }
                )
                .execute()
            )

            return bool(result.data)
        except Exception as e:
            self._log(f"âš ï¸  ä¿å­˜æ–‡ç« å¤±è´¥: {e}")
            return False

    async def process_entry(self, entry, source_info: Dict) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªRSSæ¡ç›®"""
        raw_url = entry.get("link", "")
        url = self._normalize_article_url(raw_url)
        if not url:
            return None

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        result = self.supabase.table("articles").select("id").eq("url", url).execute()
        if result.data:
            return None

        # æå–å†…å®¹
        extracted = await self.extract_content(url, source_info["anti_scraping"])
        if not extracted:
            return None

        # å‡†å¤‡æ–‡ç« æ•°æ®
        title = entry.get("title", extracted.get("title", "Untitled"))
        content = extracted.get("content", entry.get("summary", ""))

        # å¤„ç† published_parsed (time.struct_time -> ISO format)
        published_at = None
        if entry.get("published_parsed"):
            try:
                from time import mktime

                published_at = datetime.fromtimestamp(
                    mktime(entry["published_parsed"])
                ).isoformat()
            except:
                published_at = None

        article = {
            "title": title,
            "content": content,
            "url": url,
            "source_id": source_info["source_id"],
            "published_at": published_at,
            "category": source_info["category"],
            "author": entry.get("author", extracted.get("author", "")),
            "extraction_method": extracted.get("extraction_method", "local"),
            "simhash": self.compute_simhash(title + " " + content[:500]),
        }

        # æ£€æŸ¥SimHashå»é‡
        if await self.check_duplicate(article["simhash"], url):
            self.stats["articles_deduped"] += 1
            return None

        # ä¿å­˜
        if await self.save_article(article):
            self.stats["articles_new"] += 1
            return article

        return None

    async def crawl_sources(self, limit: Optional[int] = None):
        """ä¸»çˆ¬å–æµç¨‹"""
        self._log("ğŸš€ å¼€å§‹çˆ¬å–RSSæº...")

        # è·å–æ‰€æœ‰activeçš„sources
        sources = (
            self.supabase.table("rss_sources")
            .select("*")
            .eq("status", "active")
            .execute()
            .data
        )

        if limit:
            sources = sources[:limit]

        self._log(f"ğŸ“Š å…± {len(sources)} ä¸ªRSSæº")

        # åˆ›å»ºæ—¥å¿—è®°å½•
        log_result = (
            self.supabase.table("crawl_logs")
            .insert(
                {
                    "started_at": datetime.now().isoformat(),
                    "sources_count": len(sources),
                    "status": "running",
                }
            )
            .execute()
        )
        log_id = log_result.data[0]["id"] if log_result.data else None

        # å¹¶å‘æŠ“å–RSS
        tasks = [self.fetch_rss(source) for source in sources]
        rss_results = await asyncio.gather(*tasks)

        # å¤„ç†æ¯ä¸ªæºçš„æ¡ç›®
        all_entries = []
        for source, rss_data in zip(sources, rss_results):
            if rss_data and rss_data["entries"]:
                self.stats["sources_processed"] += 1
                for entry in rss_data["entries"]:
                    all_entries.append(
                        (
                            entry,
                            {
                                "source_id": source["id"],
                                "category": source["category"],
                                "anti_scraping": source.get("anti_scraping", "None"),
                            },
                        )
                    )

        total_entries = len(all_entries)
        self._log(f"ğŸ“° è·å–åˆ° {total_entries} ä¸ªæ¡ç›®ï¼Œå¼€å§‹å¤„ç†...")

        # å¤„ç†æ¡ç›®ï¼ˆé™åˆ¶å¹¶å‘ï¼‰
        semaphore = asyncio.Semaphore(10)
        progress_lock = asyncio.Lock()
        processed_entries = 0
        start_time = datetime.now()

        async def process_with_limit(entry, source_info):
            nonlocal processed_entries
            async with semaphore:
                result = await self.process_entry(entry, source_info)
            async with progress_lock:
                processed_entries += 1
                if processed_entries % 50 == 0 or processed_entries == total_entries:
                    elapsed = (datetime.now() - start_time).total_seconds()
                    rate = processed_entries / elapsed if elapsed > 0 else 0
                    self._log(
                        f"â³ æ¡ç›®å¤„ç†è¿›åº¦: {processed_entries}/{total_entries} | "
                        f"æ–°å¢: {self.stats['articles_new']} | "
                        f"å»é‡: {self.stats['articles_deduped']} | "
                        f"é€Ÿç‡: {rate:.2f} æ¡/s"
                    )
            return result

        entry_tasks = [process_with_limit(e, s) for e, s in all_entries]
        await asyncio.gather(*entry_tasks)

        # æ›´æ–°æ—¥å¿—
        if log_id:
            self.supabase.table("crawl_logs").update(
                {
                    "completed_at": datetime.now().isoformat(),
                    "articles_fetched": len(all_entries),
                    "articles_new": self.stats["articles_new"],
                    "articles_deduped": self.stats["articles_deduped"],
                    "errors_count": self.stats["errors"],
                    "status": "completed",
                }
            ).eq("id", log_id).execute()

        # æ‰“å°ç»Ÿè®¡
        self._log("=" * 60)
        self._log("ğŸ“Š çˆ¬å–å®Œæˆç»Ÿè®¡")
        self._log("=" * 60)
        self._log(f"å¤„ç†çš„æº: {self.stats['sources_processed']}")
        self._log(f"è·å–æ¡ç›®: {len(all_entries)}")
        self._log(f"æ–°å¢æ–‡ç« : {self.stats['articles_new']}")
        self._log(f"å»é‡è·³è¿‡: {self.stats['articles_deduped']}")
        self._log(f"é”™è¯¯æ•°: {self.stats['errors']}")


async def main():
    async with RSSCrawler() as crawler:
        await crawler.crawl_sources()


if __name__ == "__main__":
    asyncio.run(main())
