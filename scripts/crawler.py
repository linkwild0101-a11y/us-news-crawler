#!/usr/bin/env python3
"""
RSSçˆ¬è™«æ ¸å¿ƒ - å¼‚æ­¥æŠ“å–RSSå¹¶æå–å†…å®¹
"""

import asyncio
import aiohttp
import feedparser
import json
import os
import hashlib
from datetime import datetime
from typing import List, Dict, Optional
from supabase import create_client
from urllib.parse import urljoin, urlparse

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
WORKER_URL = os.getenv("WORKER_URL")


class RSSCrawler:
    def __init__(self):
        self.supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        self.session: Optional[aiohttp.ClientSession] = None
        self.semaphore = asyncio.Semaphore(20)  # é™åˆ¶å¹¶å‘æ•°
        self.stats = {
            "sources_processed": 0,
            "articles_fetched": 0,
            "articles_new": 0,
            "articles_deduped": 0,
            "errors": 0,
        }

    async def __aenter__(self):
        timeout = aiohttp.ClientTimeout(total=30)
        self.session = aiohttp.ClientSession(timeout=timeout)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def fetch_rss(self, source: Dict) -> Optional[Dict]:
        """æŠ“å–å•ä¸ªRSSæº"""
        async with self.semaphore:
            try:
                async with self.session.get(
                    source["rss_url"],
                    headers={"User-Agent": "Mozilla/5.0 (compatible; RSSCrawler/1.0)"},
                ) as resp:
                    if resp.status != 200:
                        raise Exception(f"HTTP {resp.status}")

                    content = await resp.text()
                    feed = feedparser.parse(content)

                    return {
                        "source_id": source["id"],
                        "category": source["category"],
                        "anti_scraping": source.get("anti_scraping", "None"),
                        "entries": feed.entries[:10],  # åªå–å‰10æ¡
                    }
            except Exception as e:
                print(f"  âš ï¸  RSSæŠ“å–å¤±è´¥ {source['name']}: {e}")
                self.stats["errors"] += 1
                return None

    async def extract_content(self, url: str, anti_scraping: str) -> Optional[Dict]:
        """æ··åˆå†…å®¹æå–"""
        try:
            if anti_scraping in ["Cloudflare", "Paywall"] and WORKER_URL:
                # ä½¿ç”¨Cloudflare Worker
                async with self.session.post(
                    f"{WORKER_URL}/extract",
                    json={"url": url},
                    headers={"Content-Type": "application/json"},
                ) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        if data.get("success"):
                            data["extraction_method"] = "cloudflare"
                            return data

            # æœ¬åœ°æå–ï¼ˆç®€åŒ–ç‰ˆï¼‰
            async with self.session.get(
                url, headers={"User-Agent": "Mozilla/5.0 (compatible; RSSCrawler/1.0)"}
            ) as resp:
                if resp.status == 200:
                    html = await resp.text()
                    # ç®€å•æå–æ ‡é¢˜å’Œæ­£æ–‡
                    title = self._extract_title(html)
                    content = self._extract_content_simple(html)
                    return {
                        "title": title,
                        "content": content,
                        "extraction_method": "local",
                    }
        except Exception as e:
            print(f"  âš ï¸  å†…å®¹æå–å¤±è´¥: {e}")

        return None

    def _extract_title(self, html: str) -> str:
        """ä»HTMLä¸­æå–æ ‡é¢˜"""
        import re

        match = re.search(r"<title[^>]*>([^<]*)</title>", html, re.I)
        return match.group(1).strip() if match else ""

    def _extract_content_simple(self, html: str) -> str:
        """ç®€å•æå–æ­£æ–‡"""
        import re

        # ç§»é™¤scriptå’Œstyle
        html = re.sub(r"<script[^>]*>[\s\S]*?</script>", "", html, flags=re.I)
        html = re.sub(r"<style[^>]*>[\s\S]*?</style>", "", html, flags=re.I)
        # æå–æ–‡æœ¬
        text = re.sub(r"<[^>]+>", " ", html)
        # æ¸…ç†
        text = " ".join(text.split())
        return text[:5000]  # é™åˆ¶é•¿åº¦

    def compute_simhash(self, text: str) -> str:
        """è®¡ç®—SimHash"""
        try:
            from simhash import Simhash

            if not text:
                return "0"
            return str(Simhash(text[:1000]))
        except ImportError:
            # å¦‚æœæ²¡æœ‰simhashåº“ï¼Œä½¿ç”¨MD5ä½œä¸ºfallback
            return hashlib.md5(text[:1000].encode()).hexdigest()

    async def check_duplicate(self, simhash: str, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦é‡å¤"""
        try:
            # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
            result = (
                self.supabase.table("articles").select("id").eq("url", url).execute()
            )
            if result.data:
                return True

            # SimHashæ£€æŸ¥ï¼ˆæœ€è¿‘7å¤©ï¼‰
            from datetime import timedelta

            cutoff = (datetime.now() - timedelta(days=7)).isoformat()

            result = (
                self.supabase.table("articles")
                .select("id, simhash")
                .gte("fetched_at", cutoff)
                .execute()
            )

            for article in result.data:
                if (
                    article.get("simhash")
                    and self._hamming_distance(simhash, article["simhash"]) <= 3
                ):
                    return True

            return False
        except Exception as e:
            print(f"  âš ï¸  å»é‡æ£€æŸ¥å¤±è´¥: {e}")
            return False

    def _hamming_distance(self, hash1: str, hash2: str) -> int:
        """è®¡ç®—æ±‰æ˜è·ç¦»"""
        try:
            h1 = int(hash1)
            h2 = int(hash2)
            x = h1 ^ h2
            distance = 0
            while x:
                distance += 1
                x &= x - 1
            return distance
        except:
            return 100  # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè¿”å›å¤§æ•°è¡¨ç¤ºä¸ç›¸ä¼¼

    async def save_article(self, article: Dict) -> bool:
        """ä¿å­˜æ–‡ç« åˆ°æ•°æ®åº“"""
        try:
            result = (
                self.supabase.table("articles")
                .insert(
                    {
                        "title": article["title"][:500],
                        "content": article.get("content", "")[:10000],
                        "url": article["url"],
                        "source_id": article["source_id"],
                        "published_at": article.get("published_at"),
                        "fetched_at": datetime.now().isoformat(),
                        "simhash": article.get("simhash"),
                        "category": article.get("category"),
                        "author": article.get("author", "")[:255],
                        "extraction_method": article.get("extraction_method", "local"),
                    }
                )
                .execute()
            )

            return bool(result.data)
        except Exception as e:
            print(f"  âš ï¸  ä¿å­˜æ–‡ç« å¤±è´¥: {e}")
            return False

    async def process_entry(self, entry, source_info: Dict) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªRSSæ¡ç›®"""
        url = entry.get("link", "")
        if not url:
            return None

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        result = self.supabase.table("articles").select("id").eq("url", url).execute()
        if result.data:
            return None

        # æå–å†…å®¹
        extracted = await self.extract_content(url, source_info["anti_scraping"])
        if not extracted:
            return None

        # å‡†å¤‡æ–‡ç« æ•°æ®
        title = entry.get("title", extracted.get("title", "Untitled"))
        content = extracted.get("content", entry.get("summary", ""))

        article = {
            "title": title,
            "content": content,
            "url": url,
            "source_id": source_info["source_id"],
            "published_at": entry.get("published_parsed"),
            "category": source_info["category"],
            "author": entry.get("author", extracted.get("author", "")),
            "extraction_method": extracted.get("extraction_method", "local"),
            "simhash": self.compute_simhash(title + " " + content[:500]),
        }

        # æ£€æŸ¥SimHashå»é‡
        if await self.check_duplicate(article["simhash"], url):
            self.stats["articles_deduped"] += 1
            return None

        # ä¿å­˜
        if await self.save_article(article):
            self.stats["articles_new"] += 1
            return article

        return None

    async def crawl_sources(self, limit: Optional[int] = None):
        """ä¸»çˆ¬å–æµç¨‹"""
        print("ğŸš€ å¼€å§‹çˆ¬å–RSSæº...")

        # è·å–æ‰€æœ‰activeçš„sources
        sources = (
            self.supabase.table("rss_sources")
            .select("*")
            .eq("status", "active")
            .execute()
            .data
        )

        if limit:
            sources = sources[:limit]

        print(f"ğŸ“Š å…± {len(sources)} ä¸ªRSSæº")

        # åˆ›å»ºæ—¥å¿—è®°å½•
        log_result = (
            self.supabase.table("crawl_logs")
            .insert(
                {
                    "started_at": datetime.now().isoformat(),
                    "sources_count": len(sources),
                    "status": "running",
                }
            )
            .execute()
        )
        log_id = log_result.data[0]["id"] if log_result.data else None

        # å¹¶å‘æŠ“å–RSS
        tasks = [self.fetch_rss(source) for source in sources]
        rss_results = await asyncio.gather(*tasks)

        # å¤„ç†æ¯ä¸ªæºçš„æ¡ç›®
        all_entries = []
        for source, rss_data in zip(sources, rss_results):
            if rss_data and rss_data["entries"]:
                self.stats["sources_processed"] += 1
                for entry in rss_data["entries"]:
                    all_entries.append(
                        (
                            entry,
                            {
                                "source_id": source["id"],
                                "category": source["category"],
                                "anti_scraping": source.get("anti_scraping", "None"),
                            },
                        )
                    )

        print(f"ğŸ“° è·å–åˆ° {len(all_entries)} ä¸ªæ¡ç›®ï¼Œå¼€å§‹å¤„ç†...")

        # å¤„ç†æ¡ç›®ï¼ˆé™åˆ¶å¹¶å‘ï¼‰
        semaphore = asyncio.Semaphore(10)

        async def process_with_limit(entry, source_info):
            async with semaphore:
                return await self.process_entry(entry, source_info)

        entry_tasks = [process_with_limit(e, s) for e, s in all_entries]
        await asyncio.gather(*entry_tasks)

        # æ›´æ–°æ—¥å¿—
        if log_id:
            self.supabase.table("crawl_logs").update(
                {
                    "completed_at": datetime.now().isoformat(),
                    "articles_fetched": len(all_entries),
                    "articles_new": self.stats["articles_new"],
                    "articles_deduped": self.stats["articles_deduped"],
                    "errors_count": self.stats["errors"],
                    "status": "completed",
                }
            ).eq("id", log_id).execute()

        # æ‰“å°ç»Ÿè®¡
        print("\n" + "=" * 60)
        print("ğŸ“Š çˆ¬å–å®Œæˆç»Ÿè®¡")
        print("=" * 60)
        print(f"å¤„ç†çš„æº: {self.stats['sources_processed']}")
        print(f"è·å–æ¡ç›®: {len(all_entries)}")
        print(f"æ–°å¢æ–‡ç« : {self.stats['articles_new']}")
        print(f"å»é‡è·³è¿‡: {self.stats['articles_deduped']}")
        print(f"é”™è¯¯æ•°: {self.stats['errors']}")


async def main():
    async with RSSCrawler() as crawler:
        await crawler.crawl_sources()


if __name__ == "__main__":
    asyncio.run(main())
