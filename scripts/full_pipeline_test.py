#!/usr/bin/env python3
"""
å®Œæ•´æµç¨‹æµ‹è¯• - åŸºäºéªŒè¯å¯ç”¨çš„æº
æµ‹è¯• RSSæŠ“å– -> å†…å®¹æå– -> SimHashå»é‡ -> æ¸…æ´— -> å…¥åº“ å…¨æµç¨‹
"""

import asyncio
import aiohttp
import feedparser
import os
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from supabase import create_client

# é…ç½®
SUPABASE_URL = os.getenv("SUPABASE_URL", "https://lwigqxyfxevldfjdeokp.supabase.co")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
WORKER_URL = os.getenv(
    "WORKER_URL", "https://content-extractor.linkwild0101.workers.dev"
)

# æµ‹è¯•é…ç½®
MAX_SOURCES = 20  # æµ‹è¯•20ä¸ªæº
MAX_ARTICLES_PER_SOURCE = 5  # æ¯ä¸ªæºæœ€å¤šå¤„ç†5ç¯‡æ–‡ç« 
CONCURRENT_LIMIT = 10  # å¹¶å‘é™åˆ¶


class FullPipelineTest:
    def __init__(self):
        if not SUPABASE_KEY:
            raise ValueError("æœªè®¾ç½® SUPABASE_KEY")

        self.supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        self.session: Optional[aiohttp.ClientSession] = None
        self.stats = {
            "sources_total": 0,
            "sources_processed": 0,
            "sources_failed": 0,
            "articles_fetched": 0,
            "articles_extracted": 0,
            "articles_deduped": 0,
            "articles_saved": 0,
            "errors": [],
        }

    async def __aenter__(self):
        timeout = aiohttp.ClientTimeout(total=30)
        self.session = aiohttp.ClientSession(timeout=timeout)
        return self

    async def __aexit__(self, *args):
        if self.session:
            await self.session.close()

    def compute_simhash(self, text: str) -> str:
        """è®¡ç®—SimHash"""
        try:
            from simhash import Simhash

            if not text:
                return "0"
            return str(Simhash(text[:1000].lower()))
        except ImportError:
            # Fallback to MD5
            return hashlib.md5(text[:1000].encode()).hexdigest()[:16]

    def hamming_distance(self, hash1: str, hash2: str) -> int:
        """è®¡ç®—æ±‰æ˜è·ç¦»"""
        try:
            h1 = int(hash1)
            h2 = int(hash2)
            x = h1 ^ h2
            distance = 0
            while x:
                distance += 1
                x &= x - 1
            return distance
        except:
            return 100

    async def check_duplicate(self, simhash: str, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦é‡å¤"""
        try:
            # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
            result = (
                self.supabase.table("articles").select("id").eq("url", url).execute()
            )
            if result.data:
                return True

            # SimHashæ£€æŸ¥ï¼ˆæœ€è¿‘7å¤©ï¼‰
            cutoff = (datetime.now() - timedelta(days=7)).isoformat()
            result = (
                self.supabase.table("articles")
                .select("id, simhash")
                .gte("fetched_at", cutoff)
                .execute()
            )

            for article in result.data:
                if article.get("simhash"):
                    if self.hamming_distance(simhash, article["simhash"]) <= 3:
                        return True

            return False
        except Exception as e:
            print(f"    âš ï¸  å»é‡æ£€æŸ¥å¤±è´¥: {e}")
            return False

    async def extract_content(self, url: str, anti_scraping: str) -> Optional[Dict]:
        """æå–æ–‡ç« å†…å®¹"""
        try:
            # å¯¹äºåçˆ¬ç«™ç‚¹ï¼Œå°è¯•ä½¿ç”¨Worker
            if anti_scraping in ["Cloudflare", "Paywall"] and WORKER_URL:
                try:
                    async with self.session.post(
                        f"{WORKER_URL}/extract",
                        json={"url": url},
                        headers={"Content-Type": "application/json"},
                        timeout=aiohttp.ClientTimeout(total=20),
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            if data.get("success"):
                                return {
                                    "title": data.get("title", ""),
                                    "content": data.get("content", ""),
                                    "excerpt": data.get("excerpt", ""),
                                    "author": data.get("author", ""),
                                    "published_time": data.get("published_time", ""),
                                    "extraction_method": "cloudflare",
                                }
                except Exception as e:
                    print(f"    âš ï¸  Workeræå–å¤±è´¥: {e}")

            # æœ¬åœ°æå–
            async with self.session.get(
                url,
                headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                },
                timeout=aiohttp.ClientTimeout(total=15),
            ) as resp:
                if resp.status == 200:
                    html = await resp.text()

                    # ç®€å•æå–
                    import re

                    title_match = re.search(r"<title[^>]*>([^<]*)</title>", html, re.I)
                    title = title_match.group(1).strip() if title_match else ""

                    # æ¸…ç†HTML
                    text = re.sub(
                        r"<script[^>]*>[\s\S]*?</script>", "", html, flags=re.I
                    )
                    text = re.sub(r"<style[^>]*>[\s\S]*?</style>", "", text, flags=re.I)
                    text = re.sub(r"<[^>]+>", " ", text)
                    text = " ".join(text.split())

                    return {
                        "title": title,
                        "content": text[:5000],
                        "excerpt": text[:200] + "..." if len(text) > 200 else text,
                        "author": "",
                        "published_time": None,
                        "extraction_method": "local",
                    }
        except Exception as e:
            print(f"    âš ï¸  å†…å®¹æå–å¤±è´¥: {str(e)[:50]}")

        return None

    def clean_text(self, text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬"""
        if not text:
            return ""

        # è§£ç HTMLå®ä½“
        import html

        text = html.unescape(text)

        # è§„èŒƒåŒ–ç©ºç™½
        text = " ".join(text.split())

        return text.strip()

    async def process_article(self, entry: Dict, source: Dict) -> bool:
        """å¤„ç†å•ç¯‡æ–‡ç« """
        url = entry.get("link", "")
        if not url:
            return False

        # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
        result = self.supabase.table("articles").select("id").eq("url", url).execute()
        if result.data:
            print(f"    â­ï¸  å·²å­˜åœ¨ï¼Œè·³è¿‡")
            return False

        # æå–å†…å®¹
        print(f"    ğŸ“ æå–å†…å®¹...")
        extracted = await self.extract_content(url, source.get("anti_scraping", "None"))
        if not extracted:
            print(f"    âŒ å†…å®¹æå–å¤±è´¥")
            return False

        self.stats["articles_extracted"] += 1

        # å‡†å¤‡æ•°æ®
        title = extracted.get("title") or entry.get("title", "Untitled")
        content = self.clean_text(extracted.get("content", ""))

        # è®¡ç®—SimHash
        simhash = self.compute_simhash(title + " " + content[:500])

        # æ£€æŸ¥é‡å¤
        print(f"    ğŸ” SimHashå»é‡æ£€æŸ¥...")
        if await self.check_duplicate(simhash, url):
            print(f"    â­ï¸  é‡å¤æ–‡ç« ï¼Œè·³è¿‡")
            self.stats["articles_deduped"] += 1
            return False

        # ä¿å­˜åˆ°æ•°æ®åº“
        print(f"    ğŸ’¾ ä¿å­˜åˆ°æ•°æ®åº“...")
        try:
            # å¤„ç† published_parsed (time.struct_time -> ISO format)
            published_at = None
            if entry.get("published_parsed"):
                try:
                    from time import mktime
                    from datetime import datetime as dt

                    published_at = dt.fromtimestamp(
                        mktime(entry["published_parsed"])
                    ).isoformat()
                except:
                    published_at = None

            article_data = {
                "title": title[:500],
                "content": content[:10000],
                "url": url,
                "source_id": source["id"],
                "published_at": published_at,
                "fetched_at": datetime.now().isoformat(),
                "simhash": simhash,
                "category": source["category"],
                "author": extracted.get("author", "")[:255],
                "summary": extracted.get("excerpt", "")[:500],
                "extraction_method": extracted.get("extraction_method", "local"),
            }

            result = self.supabase.table("articles").insert(article_data).execute()
            if result.data:
                print(f"    âœ… ä¿å­˜æˆåŠŸ")
                self.stats["articles_saved"] += 1
                return True
        except Exception as e:
            print(f"    âŒ ä¿å­˜å¤±è´¥: {e}")

        return False

    async def process_source(self, source: Dict) -> int:
        """å¤„ç†å•ä¸ªRSSæº"""
        print(f"\nğŸ“° {source['name']} ({source['category']})")
        print(f"   URL: {source['rss_url'][:60]}...")

        try:
            # æŠ“å–RSS
            async with self.session.get(
                source["rss_url"],
                headers={"User-Agent": "Mozilla/5.0 (compatible; RSSCrawler/1.0)"},
                timeout=aiohttp.ClientTimeout(total=20),
            ) as resp:
                if resp.status != 200:
                    print(f"   âŒ HTTP {resp.status}")
                    self.stats["sources_failed"] += 1
                    return 0

                content = await resp.text()
                feed = feedparser.parse(content)

                if not feed.entries:
                    print(f"   âš ï¸  æ— æ–‡ç« ")
                    return 0

                entries = feed.entries[:MAX_ARTICLES_PER_SOURCE]
                print(
                    f"   âœ… è·å– {len(feed.entries)} ç¯‡æ–‡ç« ï¼Œå¤„ç†å‰ {len(entries)} ç¯‡"
                )

                saved_count = 0
                for i, entry in enumerate(entries, 1):
                    print(
                        f"\n   [{i}/{len(entries)}] {entry.get('title', 'N/A')[:50]}..."
                    )
                    if await self.process_article(entry, source):
                        saved_count += 1
                    self.stats["articles_fetched"] += 1

                self.stats["sources_processed"] += 1
                return saved_count

        except Exception as e:
            print(f"   âŒ é”™è¯¯: {str(e)[:60]}")
            self.stats["sources_failed"] += 1
            return 0

    async def run_test(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•"""
        print("=" * 80)
        print("ğŸš€ å®Œæ•´æµç¨‹æµ‹è¯•")
        print("=" * 80)
        print(f"â±ï¸  å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()

        # è·å–éªŒè¯å¯ç”¨çš„æº
        print("ğŸ“Š è·å–éªŒè¯å¯ç”¨çš„RSSæº...")
        sources = (
            self.supabase.table("rss_sources")
            .select("*")
            .eq("status", "active")
            .execute()
            .data
        )

        if MAX_SOURCES:
            sources = sources[:MAX_SOURCES]

        self.stats["sources_total"] = len(sources)
        print(f"âœ… è·å–åˆ° {len(sources)} ä¸ªéªŒè¯å¯ç”¨çš„æº")
        print()

        # åˆ›å»ºçˆ¬å–æ—¥å¿—
        log_result = (
            self.supabase.table("crawl_logs")
            .insert(
                {
                    "started_at": datetime.now().isoformat(),
                    "sources_count": len(sources),
                    "status": "running",
                }
            )
            .execute()
        )
        log_id = log_result.data[0]["id"] if log_result.data else None

        # å¹¶å‘å¤„ç†æº
        semaphore = asyncio.Semaphore(CONCURRENT_LIMIT)

        async def process_with_limit(source):
            async with semaphore:
                return await self.process_source(source)

        tasks = [process_with_limit(s) for s in sources]
        results = await asyncio.gather(*tasks)

        total_saved = sum(results)

        # æ›´æ–°æ—¥å¿—
        if log_id:
            self.supabase.table("crawl_logs").update(
                {
                    "completed_at": datetime.now().isoformat(),
                    "articles_fetched": self.stats["articles_fetched"],
                    "articles_new": self.stats["articles_saved"],
                    "articles_deduped": self.stats["articles_deduped"],
                    "errors_count": len(self.stats["errors"]),
                    "status": "completed",
                }
            ).eq("id", log_id).execute()

        # æ‰“å°ç»Ÿè®¡
        self._print_summary(total_saved)

    def _print_summary(self, total_saved: int):
        """æ‰“å°ç»Ÿè®¡"""
        print("\n" + "=" * 80)
        print("ğŸ“Š å®Œæ•´æµç¨‹æµ‹è¯•æŠ¥å‘Š")
        print("=" * 80)
        print()
        print("ã€æºç»Ÿè®¡ã€‘")
        print(f"  æ€»æºæ•°: {self.stats['sources_total']}")
        print(f"  å¤„ç†æˆåŠŸ: {self.stats['sources_processed']}")
        print(f"  å¤„ç†å¤±è´¥: {self.stats['sources_failed']}")
        print()
        print("ã€æ–‡ç« ç»Ÿè®¡ã€‘")
        print(f"  è·å–æ–‡ç« æ•°: {self.stats['articles_fetched']}")
        print(f"  æˆåŠŸæå–: {self.stats['articles_extracted']}")
        print(f"  SimHashå»é‡: {self.stats['articles_deduped']}")
        print(f"  æˆåŠŸä¿å­˜: {self.stats['articles_saved']}")
        print()
        print("ã€æˆåŠŸç‡ã€‘")
        if self.stats["articles_fetched"] > 0:
            extraction_rate = (
                self.stats["articles_extracted"] / self.stats["articles_fetched"] * 100
            )
            save_rate = (
                self.stats["articles_saved"] / self.stats["articles_fetched"] * 100
            )
            print(f"  å†…å®¹æå–ç‡: {extraction_rate:.1f}%")
            print(f"  æœ€ç»ˆå…¥åº“ç‡: {save_rate:.1f}%")
        print()
        print(f"â±ï¸  ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)


async def main():
    async with FullPipelineTest() as tester:
        await tester.run_test()


if __name__ == "__main__":
    import sys

    # è®¾ç½®ç¯å¢ƒå˜é‡
    if not os.getenv("SUPABASE_KEY"):
        print("âŒ è¯·è®¾ç½® SUPABASE_KEY ç¯å¢ƒå˜é‡")
        sys.exit(1)

    asyncio.run(main())
