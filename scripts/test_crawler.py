#!/usr/bin/env python3
"""
çˆ¬è™«æµ‹è¯•è„šæœ¬ - é™åˆ¶å¤„ç†æ•°é‡
"""

import asyncio
import aiohttp
import feedparser
import os
from datetime import datetime
from supabase import create_client

SUPABASE_URL = os.getenv("SUPABASE_URL", "https://lwigqxyfxevldfjdeokp.supabase.co")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
WORKER_URL = os.getenv(
    "WORKER_URL", "https://content-extractor.linkwild0101.workers.dev"
)


class TestCrawler:
    def __init__(self, limit=5):
        self.supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        self.limit = limit
        self.session = None
        self.stats = {"processed": 0, "articles": 0, "errors": 0}

    async def __aenter__(self):
        self.session = aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30))
        return self

    async def __aexit__(self, *args):
        if self.session:
            await self.session.close()

    async def test_crawl(self):
        print("ğŸš€ å¼€å§‹æµ‹è¯•çˆ¬å–...")
        print(f"ğŸ“Š é™åˆ¶å¤„ç†: {self.limit} ä¸ªæº\n")

        # è·å–RSSæº
        sources = (
            self.supabase.table("rss_sources")
            .select("*")
            .eq("status", "active")
            .limit(self.limit)
            .execute()
            .data
        )
        print(f"âœ… è·å–åˆ° {len(sources)} ä¸ªæº\n")

        for source in sources:
            print(f"ğŸ“° å¤„ç†: {source['name']} ({source['category']})")
            try:
                # æŠ“å–RSS
                async with self.session.get(
                    source["rss_url"],
                    headers={"User-Agent": "Mozilla/5.0 (compatible; TestCrawler/1.0)"},
                ) as resp:
                    if resp.status != 200:
                        print(f"   âš ï¸  HTTP {resp.status}")
                        continue

                    content = await resp.text()
                    feed = feedparser.parse(content)

                    if not feed.entries:
                        print(f"   â„¹ï¸  æ— æ–‡ç« ")
                        continue

                    entry = feed.entries[0]  # åªæµ‹è¯•ç¬¬ä¸€æ¡
                    print(
                        f"   âœ… è·å–RSSæˆåŠŸï¼Œæ–‡ç« : {entry.get('title', 'N/A')[:50]}..."
                    )

                    # æµ‹è¯•å†…å®¹æå–
                    url = entry.get("link", "")
                    if url:
                        extracted = await self.extract_content(
                            url, source.get("anti_scraping", "None")
                        )
                        if extracted:
                            print(
                                f"   âœ… å†…å®¹æå–æˆåŠŸ ({extracted.get('extraction_method', 'local')})"
                            )
                            print(
                                f"   ğŸ“ æ ‡é¢˜: {extracted.get('title', 'N/A')[:40]}..."
                            )
                            self.stats["articles"] += 1
                        else:
                            print(f"   âš ï¸  å†…å®¹æå–å¤±è´¥")

                    self.stats["processed"] += 1

            except Exception as e:
                print(f"   âŒ é”™è¯¯: {str(e)[:60]}")
                self.stats["errors"] += 1

            print()

        print("=" * 60)
        print("ğŸ“Š æµ‹è¯•ç»Ÿè®¡")
        print("=" * 60)
        print(f"å¤„ç†çš„æº: {self.stats['processed']}")
        print(f"æˆåŠŸæå–: {self.stats['articles']}")
        print(f"é”™è¯¯æ•°: {self.stats['errors']}")
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")

    async def extract_content(self, url, anti_scraping):
        try:
            # å°è¯•æœ¬åœ°æå–
            async with self.session.get(
                url,
                headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                },
            ) as resp:
                if resp.status == 200:
                    html = await resp.text()
                    # ç®€å•æå–æ ‡é¢˜
                    import re

                    title_match = re.search(r"<title[^>]*>([^<]*)</title>", html, re.I)
                    title = title_match.group(1).strip() if title_match else ""
                    return {"title": title, "extraction_method": "local"}
        except Exception as e:
            print(f"   âš ï¸  æœ¬åœ°æå–å¤±è´¥: {str(e)[:40]}")

        return None


async def main():
    async with TestCrawler(limit=5) as crawler:
        await crawler.test_crawl()


if __name__ == "__main__":
    import sys

    # è®¾ç½®ç¯å¢ƒå˜é‡
    if not os.getenv("SUPABASE_KEY"):
        os.environ["SUPABASE_KEY"] = (
            "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Imx3aWdxeHlmeGV2bGRmamRlb2twIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc3MTE3MzYxMiwiZXhwIjoyMDg2NzQ5NjEyfQ.-JCEODgYe83EugQeTxLHsxBXikXbz_btei9-qsUxb1M"
        )

    asyncio.run(main())
