#!/usr/bin/env python3
"""
US-Monitor UI ä»ªè¡¨æ¿
ä½¿ç”¨ Streamlit æ„å»ºä¸­æ–‡ç•Œé¢
"""

import os
import sys
import json
import logging
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import streamlit as st
import pandas as pd
from supabase import create_client

from config.entity_config import (
    ENTITY_TYPES,
    PERSON_RULES,
    DETECTION_PRIORITY,
)

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not logger.handlers:
    formatter = logging.Formatter(
        "%(asctime)s [%(levelname)s] %(name)s: %(message)s", datefmt="%Y-%m-%d %H:%M:%S"
    )
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="US-Monitor çƒ­ç‚¹åˆ†æ",
    page_icon="ğŸ‡ºğŸ‡¸",
    layout="wide",
    initial_sidebar_state="expanded",
)

# ä¸»é¢˜é…ç½® - ç®€æ´é»‘ç™½é£ï¼Œé«˜å¯¹æ¯”åº¦
THEME = {
    "bg_main": "#111111",
    "bg_card": "#1a1a1a",
    "bg_sidebar": "#0d0d0d",
    "text_main": "#ffffff",
    "text_body": "#e0e0e0",
    "text_muted": "#888888",
    "primary": "#ffffff",
    "accent": "#00ff88",
    "border": "#333333",
}


# ç”Ÿæˆ CSS
def get_css():
    t = THEME
    return f"""
<style>
    /* Streamlit å…¨å±€è¦†ç›– */
    .stApp {{
        background: {t["bg_main"]};
    }}
    
    /* ä¾§è¾¹æ  */
    [data-testid="stSidebar"] {{
        background: {t["bg_sidebar"]};
    }}
    
    /* æ‰€æœ‰æ–‡å­—ç™½è‰²é«˜äº® */
    .stMarkdown, .stText, p, h1, h2, h3, h4, h5, h6, label {{
        color: {t["text_main"]} !important;
    }}
    
    /* ä¸»æ ‡é¢˜ */
    .main-header {{
        font-size: 2rem;
        font-weight: 700;
        color: {t["text_main"]};
        margin-bottom: 1.5rem;
        border-bottom: 2px solid {t["accent"]};
        padding-bottom: 0.5rem;
    }}
    
    /* æŒ‡æ ‡å¡ç‰‡ */
    [data-testid="stMetric"] {{
        background: {t["bg_card"]};
        padding: 1rem;
        border-radius: 8px;
        border: 1px solid {t["border"]};
    }}
    
    [data-testid="stMetricLabel"] {{
        color: {t["text_muted"]} !important;
    }}
    
    [data-testid="stMetricValue"] {{
        color: {t["text_main"]} !important;
    }}
    
    /* çƒ­ç‚¹å¡ç‰‡ */
    .hotspot-card {{
        background: {t["bg_card"]};
        padding: 1.25rem;
        border-radius: 8px;
        border: 1px solid {t["border"]};
        margin-bottom: 1rem;
        border-left: 3px solid {t["accent"]};
    }}
    
    .hotspot-card h4 {{
        color: {t["text_main"]};
        font-weight: 600;
        font-size: 1.1rem;
        margin-bottom: 0.75rem;
    }}
    
    .hotspot-card h5 {{
        color: {t["text_body"]};
        font-weight: 600;
    }}
    
    .hotspot-card p {{
        color: {t["text_body"]};
        font-size: 0.95rem;
    }}
    
    .hotspot-card .meta-text {{
        color: {t["text_muted"]};
        font-size: 0.85rem;
    }}
    
    /* ä¿¡å·å¾½ç«  */
    .signal-high {{
        color: #ff6b6b;
    }}
    .signal-medium {{
        color: #ffd93d;
    }}
    .signal-low {{
        color: #6bcb77;
    }}
    
    /* åˆ†å‰²çº¿ */
    hr {{
        border: none;
        height: 1px;
        background: {t["border"]};
        margin: 1.5rem 0;
    }}
    
    /* æ»šåŠ¨æ¡ */
    ::-webkit-scrollbar {{
        width: 6px;
    }}
    ::-webkit-scrollbar-track {{
        background: {t["bg_main"]};
    }}
    ::-webkit-scrollbar-thumb {{
        background: {t["border"]};
    }}
    
    /* æŒ‰é’® */
    .stButton > button {{
        background: {t["bg_card"]};
        color: {t["text_main"]};
        border: 1px solid {t["border"]};
    }}
    .stButton > button:hover {{
        background: {t["border"]};
    }}
    
    /* ä¸‹æ‹‰æ¡† */
    .stSelectbox > div > div {{
        background: {t["bg_card"]};
        color: {t["text_main"]};
    }}
    
    /* å•é€‰æŒ‰é’® */
    .stRadio > div {{
        color: {t["text_body"]};
    }}
    
    /* é“¾æ¥æŒ‰é’® */
    .stLinkButton > button {{
        background: transparent;
        border: 1px solid {t["accent"]};
        color: {t["accent"]} !important;
    }}
    .stLinkButton > button:hover {{
        background: {t["accent"]};
        color: {t["bg_main"]} !important;
    }}
    
    /* å±•å¼€æ¡† */
    .streamlit-expanderHeader {{
        color: {t["text_body"]} !important;
        background: {t["bg_card"]};
    }}
    
    /* è¡¨æ ¼ */
    .stDataFrame {{
        background: {t["bg_card"]};
    }}
    
    /* å›¾è¡¨ */
    [data-testid="stChart"] {{
        background: {t["bg_card"]};
    }}
    
    /* åˆ†ç±»æ ‡ç­¾ */
    .category-military {{ color: #ff6b6b; font-weight: 600; }}
    .category-politics {{ color: #a78bfa; font-weight: 600; }}
    .category-economy {{ color: #6bcb77; font-weight: 600; }}
    
    /* å¿«é€Ÿç¿»è¯‘å¾½ç«  */
    .shallow-badge {{
        background: #ffd93d;
        color: #1a1a1a;
        padding: 2px 8px;
        border-radius: 4px;
        font-size: 0.75rem;
        margin-left: 8px;
    }}
</style>
"""


# åº”ç”¨ä¸»é¢˜ CSS
st.markdown(get_css(), unsafe_allow_html=True)


# åˆå§‹åŒ– Supabase
@st.cache_resource
def init_supabase():
    """åˆå§‹åŒ– Supabase å®¢æˆ·ç«¯"""
    url = os.getenv("SUPABASE_URL")
    key = os.getenv("SUPABASE_KEY")

    if not url or not key:
        st.error("ç¼ºå°‘ Supabase é…ç½®ã€‚è¯·è®¾ç½® SUPABASE_URL å’Œ SUPABASE_KEY ç¯å¢ƒå˜é‡ã€‚")
        return None

    return create_client(url, key)


# æ•°æ®è·å–å‡½æ•°
@st.cache_data(ttl=300)  # ç¼“å­˜5åˆ†é’Ÿ
def get_clusters(_supabase, hours: int = 24, category: str = None) -> pd.DataFrame:
    """è·å–èšç±»æ•°æ®"""
    cutoff = (datetime.now() - timedelta(hours=hours)).isoformat()

    query = _supabase.table("analysis_clusters").select("*").gte("created_at", cutoff)

    if category and category != "å…¨éƒ¨":
        query = query.eq("category", category)

    result = query.order("created_at", desc=True).execute()

    if result.data:
        return pd.DataFrame(result.data)
    return pd.DataFrame()


@st.cache_data(ttl=300)
def get_signals(_supabase, hours: int = 24) -> pd.DataFrame:
    """è·å–ä¿¡å·æ•°æ®"""
    cutoff = (datetime.now() - timedelta(hours=hours)).isoformat()

    result = (
        _supabase.table("analysis_signals")
        .select("*")
        .gte("created_at", cutoff)
        .order("confidence", desc=True)
        .execute()
    )

    if result.data:
        return pd.DataFrame(result.data)
    return pd.DataFrame()


@st.cache_data(ttl=300)
def get_stats(_supabase) -> dict:
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    today = datetime.now().strftime("%Y-%m-%d")

    # ä»Šæ—¥èšç±»æ•°
    clusters_today = (
        _supabase.table("analysis_clusters")
        .select("*", count="exact")
        .gte("created_at", today)
        .execute()
    )

    # ä»Šæ—¥ä¿¡å·æ•°
    signals_today = (
        _supabase.table("analysis_signals")
        .select("*", count="exact")
        .gte("created_at", today)
        .execute()
    )

    # æ€»æ–‡ç« æ•°
    articles_total = _supabase.table("articles").select("*", count="exact").execute()

    # æœªåˆ†ææ–‡ç« æ•°
    articles_unanalyzed = (
        _supabase.table("articles")
        .select("*", count="exact")
        .is_("analyzed_at", "null")
        .execute()
    )

    return {
        "clusters_today": clusters_today.count,
        "signals_today": signals_today.count,
        "articles_total": articles_total.count,
        "articles_unanalyzed": articles_unanalyzed.count,
    }


# ä¾§è¾¹æ 
def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ """
    st.sidebar.markdown("## ğŸ“Š å¯¼èˆª")

    page = st.sidebar.radio(
        "é€‰æ‹©é¡µé¢:",
        ["ğŸ  æ¦‚è§ˆé¦–é¡µ", "ğŸ”¥ çƒ­ç‚¹è¯¦æƒ…", "ğŸ“¡ ä¿¡å·ä¸­å¿ƒ", "ğŸ“ å®ä½“æ¡£æ¡ˆ", "ğŸ“ˆ æ•°æ®ç»Ÿè®¡"],
    )

    st.sidebar.markdown("---")
    st.sidebar.markdown("### âš™ï¸ è®¾ç½®")

    time_range = st.sidebar.selectbox("æ—¶é—´èŒƒå›´:", ["24å°æ—¶", "7å¤©", "30å¤©"], index=0)

    hours_map = {"24å°æ—¶": 24, "7å¤©": 168, "30å¤©": 720}

    category = st.sidebar.selectbox(
        "åˆ†ç±»ç­›é€‰:", ["å…¨éƒ¨", "military", "politics", "economy"]
    )

    return page, hours_map[time_range], category


# æ¦‚è§ˆé¦–é¡µ
def render_overview(supabase, hours: int, category: str):
    """æ¸²æŸ“æ¦‚è§ˆé¦–é¡µ"""
    st.markdown(
        '<div class="main-header">ğŸ‡ºğŸ‡¸ US-Monitor çƒ­ç‚¹åˆ†æ</div>', unsafe_allow_html=True
    )

    # è·å–ç»Ÿè®¡æ•°æ®
    stats = get_stats(supabase)

    # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("ä»Šæ—¥èšç±»", stats["clusters_today"])
    with col2:
        st.metric("ä»Šæ—¥ä¿¡å·", stats["signals_today"])
    with col3:
        st.metric("æ€»æ–‡ç« æ•°", f"{stats['articles_total']:,}")
    with col4:
        st.metric("å¾…åˆ†æ", stats["articles_unanalyzed"])

    st.markdown("---")

    # è·å–æ•°æ®
    clusters_df = get_clusters(supabase, hours, category)
    signals_df = get_signals(supabase, hours)

    # æœ€æ–°çƒ­ç‚¹
    st.markdown("### ğŸ”¥ æœ€æ–°çƒ­ç‚¹ (TOP 5)")

    if clusters_df.empty:
        st.info("æš‚æ— çƒ­ç‚¹æ•°æ®")
    else:
        for idx, row in clusters_df.head(5).iterrows():
            with st.container():
                # åˆ¤æ–­åˆ†ææ·±åº¦
                is_shallow = row.get("analysis_depth") == "shallow"
                depth_badge = (
                    "<span style='background-color: rgba(245, 158, 11, 0.3); color: #fbbf24; padding: 2px 8px; border-radius: 4px; font-size: 0.75rem; margin-left: 8px;'>å¿«é€Ÿç¿»è¯‘</span>"
                    if is_shallow
                    else ""
                )

                st.markdown(
                    f"""
                <div class="hotspot-card">
                    <h4>{row.get("primary_title", "N/A")[:80]}...{depth_badge}</h4>
                    <p><strong>ä¸­æ–‡æ‘˜è¦:</strong> {row.get("summary", "N/A")[:150]}...</p>
                    <p class="meta-text">
                        ğŸ“ {row.get("category", "N/A")} |
                        ğŸ“„ {row.get("article_count", 0)} ç¯‡æ–‡ç«  |
                        â° {row.get("created_at", "N/A")[:10]}
                    </p>
                </div>
                """,
                    unsafe_allow_html=True,
                )

                # æ ¹æ®åˆ†ææ·±åº¦æ˜¾ç¤ºä¸åŒæŒ‰é’®
                col1, col2 = st.columns([1, 1])
                with col1:
                    if row.get("primary_link"):
                        st.link_button("ğŸ”— æŸ¥çœ‹è‹±æ–‡åŸæ–‡", row["primary_link"])
                with col2:
                    if is_shallow:
                        # æµ…å±‚åˆ†ææ˜¾ç¤ºæ·±åº¦åˆ†ææŒ‰é’®
                        if st.button(
                            f"ğŸ” æ·±åº¦åˆ†æ", key=f"deep_analysis_{row.get('id')}"
                        ):
                            with st.spinner("æ­£åœ¨è¿›è¡Œæ·±åº¦åˆ†æï¼Œè¯·ç¨å€™..."):
                                try:
                                    # è°ƒç”¨åç«¯APIè¿›è¡Œæ·±åº¦åˆ†æ
                                    result = trigger_deep_analysis(
                                        supabase, row.get("id")
                                    )
                                    if result:
                                        st.success("âœ… æ·±åº¦åˆ†æå®Œæˆï¼")
                                        st.rerun()
                                    else:
                                        st.error("âŒ åˆ†æå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•")
                                except Exception as e:
                                    st.error(f"âŒ åˆ†æå‡ºé”™: {str(e)}")

    # æœ€æ–°ä¿¡å·
    st.markdown("### ğŸ“¡ æœ€æ–°ä¿¡å·")

    if signals_df.empty:
        st.info("æš‚æ— ä¿¡å·æ•°æ®")
    else:
        for idx, row in signals_df.head(5).iterrows():
            confidence = row.get("confidence", 0)
            if confidence >= 0.8:
                level_class = "signal-high"
                level_text = "é«˜"
            elif confidence >= 0.6:
                level_class = "signal-medium"
                level_text = "ä¸­"
            else:
                level_class = "signal-low"
                level_text = "ä½"

            # è·å–ä¿¡å·åç§°ï¼Œå¦‚æœæ²¡æœ‰nameå­—æ®µï¼Œä½¿ç”¨signal_typeè½¬æ¢
            signal_name = row.get("name")
            if not signal_name or signal_name == "N/A":
                signal_type = row.get("signal_type", "unknown")
                # ä¿¡å·ç±»å‹åˆ°ä¸­æ–‡åç§°çš„æ˜ å°„
                type_names = {
                    "velocity_spike": "ğŸš€ é€Ÿåº¦æ¿€å¢",
                    "convergence": "ğŸ”„ å¤šæºèšåˆ",
                    "triangulation": "ğŸ“ ä¸‰è§’éªŒè¯",
                    "hotspot_escalation": "ğŸ”¥ çƒ­ç‚¹å‡çº§",
                    "economic_indicator_alert": "ğŸ“Š ç»æµæŒ‡æ ‡å¼‚å¸¸",
                    "natural_disaster_signal": "ğŸŒ‹ è‡ªç„¶ç¾å®³",
                    "geopolitical_intensity": "ğŸŒ åœ°ç¼˜æ”¿æ²»ç´§å¼ ",
                }
                signal_name = type_names.get(signal_type, f"âš¡ {signal_type}")

            st.markdown(
                f"""
            <div class="hotspot-card">
                <h5>
                    {row.get("icon", "âš¡")} {signal_name}
                    <span class="signal-badge {level_class}">{level_text} ç½®ä¿¡åº¦</span>
                </h5>
                <p>{row.get("description", "N/A")[:100]}...</p>
                <p class="meta-text">
                    ç½®ä¿¡åº¦: {confidence:.2f} | æ—¶é—´: {row.get("created_at", "N/A")[:16]}
                </p>
            </div>
            """,
                unsafe_allow_html=True,
            )


# çƒ­ç‚¹è¯¦æƒ…é¡µ
def render_hotspots(supabase, hours: int, category: str):
    """æ¸²æŸ“çƒ­ç‚¹è¯¦æƒ…é¡µ"""
    st.markdown('<div class="main-header">ğŸ”¥ çƒ­ç‚¹è¯¦æƒ…</div>', unsafe_allow_html=True)

    clusters_df = get_clusters(supabase, hours, category)

    if clusters_df.empty:
        st.info("æš‚æ— çƒ­ç‚¹æ•°æ®")
        return

    # åˆ†ç±»æ ‡ç­¾
    tabs = st.tabs(["å…¨éƒ¨", "å†›äº‹", "æ”¿æ²»", "ç»æµ"])
    categories = [None, "military", "politics", "economy"]

    for tab, cat in zip(tabs, categories):
        with tab:
            if cat:
                filtered_df = clusters_df[clusters_df["category"] == cat]
            else:
                filtered_df = clusters_df

            st.write(f"å…± {len(filtered_df)} ä¸ªçƒ­ç‚¹")

            for idx, row in filtered_df.iterrows():
                with st.expander(f"ğŸ“° {row.get('primary_title', 'N/A')[:60]}..."):
                    st.markdown(f"**ä¸­æ–‡æ‘˜è¦:**")
                    st.write(row.get("summary", "N/A"))

                    st.markdown(f"**å…³é”®å®ä½“:**")
                    try:
                        entities = eval(row.get("key_entities", "[]"))
                        if entities:
                            st.write(", ".join(entities))
                        else:
                            st.write("æ— ")
                    except:
                        st.write("æ— ")

                    st.markdown(f"**å½±å“åˆ†æ:**")
                    st.write(row.get("impact", "æš‚æ— åˆ†æ"))

                    st.markdown(f"**è¶‹åŠ¿åˆ¤æ–­:**")
                    st.write(row.get("trend", "æš‚æ— åˆ¤æ–­"))

                    col1, col2 = st.columns(2)
                    with col1:
                        st.write(f"ğŸ“ åˆ†ç±»: {row.get('category', 'N/A')}")
                        st.write(f"ğŸ“„ æ–‡ç« æ•°: {row.get('article_count', 0)}")
                    with col2:
                        st.write(f"â° åˆ›å»ºæ—¶é—´: {row.get('created_at', 'N/A')[:16]}")
                        if row.get("primary_link"):
                            st.link_button("ğŸ”— æŸ¥çœ‹åŸæ–‡", row["primary_link"])


# ä¿¡å·ä¸­å¿ƒé¡µ
def render_signals(supabase, hours: int):
    """æ¸²æŸ“ä¿¡å·ä¸­å¿ƒé¡µ"""
    st.markdown('<div class="main-header">ğŸ“¡ ä¿¡å·ä¸­å¿ƒ</div>', unsafe_allow_html=True)

    signals_df = get_signals(supabase, hours)

    if signals_df.empty:
        st.info("æš‚æ— ä¿¡å·æ•°æ®")
        return

    # ä¿¡å·ç±»å‹ç­›é€‰
    signal_types = signals_df["signal_type"].unique().tolist()
    selected_type = st.selectbox("ä¿¡å·ç±»å‹:", ["å…¨éƒ¨"] + signal_types)

    if selected_type != "å…¨éƒ¨":
        signals_df = signals_df[signals_df["signal_type"] == selected_type]

    # ç½®ä¿¡åº¦ç­›é€‰
    min_confidence = st.slider("æœ€å°ç½®ä¿¡åº¦:", 0.0, 1.0, 0.5, 0.1)
    signals_df = signals_df[signals_df["confidence"] >= min_confidence]

    st.write(f"å…± {len(signals_df)} ä¸ªä¿¡å·")

    # æ˜¾ç¤ºä¿¡å·åˆ—è¡¨
    for idx, row in signals_df.iterrows():
        confidence = row.get("confidence", 0)

        if confidence >= 0.8:
            level_color = "#ff4b4b"
        elif confidence >= 0.6:
            level_color = "#ffa500"
        else:
            level_color = "#4caf50"

        # è·å–ä¿¡å·åç§°ï¼Œå¦‚æœæ²¡æœ‰nameå­—æ®µï¼Œä½¿ç”¨signal_typeè½¬æ¢
        signal_name = row.get("name")
        if not signal_name or signal_name == "N/A":
            signal_type = row.get("signal_type", "unknown")
            # ä¿¡å·ç±»å‹åˆ°ä¸­æ–‡åç§°çš„æ˜ å°„
            type_names = {
                "velocity_spike": "ğŸš€ é€Ÿåº¦æ¿€å¢",
                "convergence": "ğŸ”„ å¤šæºèšåˆ",
                "triangulation": "ğŸ“ ä¸‰è§’éªŒè¯",
                "hotspot_escalation": "ğŸ”¥ çƒ­ç‚¹å‡çº§",
                "economic_indicator_alert": "ğŸ“Š ç»æµæŒ‡æ ‡å¼‚å¸¸",
                "natural_disaster_signal": "ğŸŒ‹ è‡ªç„¶ç¾å®³",
                "geopolitical_intensity": "ğŸŒ åœ°ç¼˜æ”¿æ²»ç´§å¼ ",
            }
            signal_name = type_names.get(signal_type, f"âš¡ {signal_type}")

        st.markdown(
            f"""
        <div class="hotspot-card" style="border-left: 4px solid {level_color};">
            <h4>{row.get("icon", "âš¡")} {signal_name}</h4>
            <p>{row.get("description", "N/A")}</p>
            <p>
                <span style="color: {level_color}; font-weight: bold;">
                    ç½®ä¿¡åº¦: {confidence:.2f}
                </span> |
                <span class="meta-text">æ—¶é—´: {row.get("created_at", "N/A")[:16]}</span>
            </p>
        </div>
        """,
            unsafe_allow_html=True,
        )

    # ä¿¡å·ç»Ÿè®¡å›¾è¡¨
    if not signals_df.empty:
        st.markdown("### ğŸ“Š ä¿¡å·ç»Ÿè®¡")

        col1, col2 = st.columns(2)

        with col1:
            # æŒ‰ç±»å‹ç»Ÿè®¡
            type_counts = signals_df["signal_type"].value_counts()
            st.bar_chart(type_counts)

        with col2:
            # æŒ‰ç½®ä¿¡åº¦åˆ†å¸ƒ
            conf_dist = pd.cut(
                signals_df["confidence"],
                bins=[0, 0.6, 0.8, 1.0],
                labels=["ä½", "ä¸­", "é«˜"],
            ).value_counts()
            st.bar_chart(conf_dist)


# å®ä½“ç®¡ç†å‡½æ•°
def get_cluster_articles(supabase, cluster_id: int) -> list:
    """è·å–èšç±»å…³è”çš„æ‰€æœ‰æ–‡ç« """
    try:
        # è·å–å…³è”çš„æ–‡ç« ID
        relations = (
            supabase.table("article_analyses")
            .select("article_id")
            .eq("cluster_id", cluster_id)
            .execute()
        )

        if not relations.data:
            return []

        article_ids = [r["article_id"] for r in relations.data]

        # è·å–æ–‡ç« è¯¦æƒ…
        articles = []
        for aid in article_ids:
            result = (
                supabase.table("articles")
                .select("id, title, content, url, category")
                .eq("id", aid)
                .execute()
            )
            if result.data:
                articles.append(result.data[0])

        return articles
    except Exception as e:
        logger.error(f"è·å–èšç±»æ–‡ç« å¤±è´¥: {e}")
        return []


def _detect_entity_type(entity_name: str) -> str:
    """
    æ£€æµ‹å®ä½“ç±»å‹

    ä»é…ç½®æ–‡ä»¶è¯»å–å…³é”®è¯è¿›è¡Œæ£€æµ‹

    Args:
        entity_name: å®ä½“åç§°

    Returns:
        å®ä½“ç±»å‹: person/organization/location/event/concept
    """
    name = entity_name.strip()

    # æŒ‰ä¼˜å…ˆçº§æ£€æµ‹
    for entity_type in DETECTION_PRIORITY:
        if entity_type == "concept":
            continue

        if entity_type == "person":
            # äººåç‰¹æ®Šå¤„ç†
            rules = PERSON_RULES
            name_len = len(name)
            min_len = rules["chinese_name_length"]["min"]
            max_len = rules["chinese_name_length"]["max"]

            # ä¸­æ–‡äººåé•¿åº¦åˆ¤æ–­
            if min_len <= name_len <= max_len:
                return "person"

            # è‹±æ–‡äººååˆ¤æ–­
            indicators = rules["english_indicators"]
            if "contains_space" in indicators and " " in name:
                return "person"
            if "title_capitalized" in indicators and name and name[0].isupper():
                # æ£€æŸ¥æ˜¯å¦æ˜¯å¸¸è§è‹±æ–‡å
                common_names = rules.get("common_english_names", [])
                name_parts = name.split()
                for part in name_parts:
                    if part in common_names:
                        return "person"

            continue

        # å…¶ä»–ç±»å‹ï¼šä»é…ç½®è¯»å–å…³é”®è¯
        config = ENTITY_TYPES.get(entity_type, {})
        keywords_config = config.get("keywords", {})

        # åˆå¹¶ä¸­è‹±æ–‡å…³é”®è¯
        all_keywords = []
        all_keywords.extend(keywords_config.get("zh", []))
        all_keywords.extend(keywords_config.get("en", []))

        # æ£€æŸ¥å…³é”®è¯åŒ¹é…
        for keyword in all_keywords:
            if keyword in name:
                return entity_type

    # é»˜è®¤ä¸ºæ¦‚å¿µ
    return "concept"


def update_entities(supabase, cluster_id: int, entities: list, category: str):
    """æ›´æ–°å®ä½“è¡¨å’Œå®ä½“-èšç±»å…³è”è¡¨"""
    try:
        for entity_name in entities:
            if not entity_name or len(entity_name) < 2:
                continue

            # è‡ªåŠ¨æ£€æµ‹å®ä½“ç±»å‹
            entity_type = _detect_entity_type(entity_name)

            # æ£€æŸ¥å®ä½“æ˜¯å¦å·²å­˜åœ¨
            existing = (
                supabase.table("entities")
                .select("id, mention_count_total")
                .eq("name", entity_name)
                .execute()
            )

            if existing.data:
                # æ›´æ–°ç°æœ‰å®ä½“
                entity_id = existing.data[0]["id"]
                new_count = existing.data[0]["mention_count_total"] + 1

                supabase.table("entities").update(
                    {
                        "last_seen": datetime.now().isoformat(),
                        "mention_count_total": new_count,
                        "category": category,
                    }
                ).eq("id", entity_id).execute()
            else:
                # åˆ›å»ºæ–°å®ä½“
                result = (
                    supabase.table("entities")
                    .insert(
                        {
                            "name": entity_name,
                            "entity_type": entity_type,
                            "category": category,
                            "mention_count_total": 1,
                        }
                    )
                    .execute()
                )
                entity_id = result.data[0]["id"]

            # åˆ›å»ºæˆ–æ›´æ–°å®ä½“-èšç±»å…³è”
            try:
                supabase.table("entity_cluster_relations").upsert(
                    {
                        "entity_id": entity_id,
                        "cluster_id": cluster_id,
                        "mention_count": 1,
                    }
                ).execute()
            except Exception as e:
                logger.warning(f"å®ä½“å…³è”åˆ›å»ºå¤±è´¥ï¼ˆå¯èƒ½å·²å­˜åœ¨ï¼‰: {e}")

        logger.info(f"å®ä½“æ›´æ–°å®Œæˆ: {len(entities)} ä¸ªå®ä½“")
    except Exception as e:
        logger.error(f"æ›´æ–°å®ä½“å¤±è´¥: {e}")


def trigger_deep_analysis(supabase, cluster_id: int) -> bool:
    """
    è§¦å‘å¯¹æµ…å±‚åˆ†æèšç±»çš„æ·±åº¦åˆ†æ

    Args:
        supabase: Supabase å®¢æˆ·ç«¯
        cluster_id: èšç±»ID

    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    try:
        import sys
        import os

        # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
        sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

        from scripts.llm_client import LLMClient
        from config.analysis_config import LLM_PROMPTS

        logger.info(f"å¼€å§‹æ·±åº¦åˆ†æèšç±» {cluster_id}")

        # 1. è·å–èšç±»ä¿¡æ¯
        cluster_result = (
            supabase.table("analysis_clusters")
            .select("*")
            .eq("id", cluster_id)
            .execute()
        )

        if not cluster_result.data:
            logger.error(f"èšç±» {cluster_id} ä¸å­˜åœ¨")
            return False

        cluster = cluster_result.data[0]

        # 2. è·å–å…³è”çš„æ–‡ç« 
        articles = get_cluster_articles(supabase, cluster_id)

        if not articles:
            logger.warning(f"èšç±» {cluster_id} æ²¡æœ‰å…³è”æ–‡ç« ")
            # ä»ç„¶å°è¯•åˆ†æï¼Œä½¿ç”¨å·²æœ‰æ ‡é¢˜
            articles = [{"title": cluster["primary_title"], "content": ""}]

        # 3. å‡†å¤‡åˆ†ææ•°æ®
        titles = [a["title"] for a in articles]
        content_samples = "\n".join([a.get("content", "")[:500] for a in articles[:3]])

        # 4. è°ƒç”¨LLMè¿›è¡Œå®Œæ•´åˆ†æ
        llm_client = LLMClient()

        prompt = LLM_PROMPTS["cluster_summary"].format(
            article_count=cluster["article_count"],
            sources=", ".join(titles[:5]),
            primary_title=cluster["primary_title"],
            content_samples=content_samples[:1000],
        )

        logger.info(f"è°ƒç”¨LLMè¿›è¡Œæ·±åº¦åˆ†æ...")
        result = llm_client.summarize(prompt, model="qwen-plus")

        # 5. æ›´æ–°èšç±»æ•°æ®
        update_data = {
            "summary": result.get("summary", cluster["primary_title"]),
            "key_entities": json.dumps(result.get("key_entities", [])),
            "impact": result.get("impact", ""),
            "trend": result.get("trend", ""),
            "analysis_depth": "full",
            "full_analysis_triggered": True,
            "is_hot": cluster["article_count"] >= 3,
            "updated_at": datetime.now().isoformat(),
        }

        supabase.table("analysis_clusters").update(update_data).eq(
            "id", cluster_id
        ).execute()

        # 6. æ›´æ–°å®ä½“è¿½è¸ª
        entities = result.get("key_entities", [])
        if entities:
            update_entities(supabase, cluster_id, entities, cluster["category"])

        logger.info(f"æ·±åº¦åˆ†æå®Œæˆ: èšç±» {cluster_id}")
        return True

    except Exception as e:
        logger.error(f"æ·±åº¦åˆ†æå¤±è´¥: {e}")
        return False


# å®ä½“æ•°æ®è·å–å‡½æ•°
@st.cache_data(ttl=300)
def get_entities(
    _supabase, entity_type: str = None, category: str = None, limit: int = 50
) -> pd.DataFrame:
    """è·å–å®ä½“åˆ—è¡¨"""
    query = _supabase.table("entities").select("*")

    if entity_type and entity_type != "å…¨éƒ¨":
        query = query.eq("entity_type", entity_type)
    if category and category != "å…¨éƒ¨":
        query = query.eq("category", category)

    result = query.order("mention_count_total", desc=True).limit(limit).execute()

    if result.data:
        return pd.DataFrame(result.data)
    return pd.DataFrame()


@st.cache_data(ttl=300)
def get_entity_related_clusters(
    _supabase, entity_id: int, limit: int = 10
) -> pd.DataFrame:
    """è·å–å®ä½“å…³è”çš„èšç±»"""
    # è·å–å…³è”çš„èšç±»ID
    relations = (
        _supabase.table("entity_cluster_relations")
        .select("cluster_id")
        .eq("entity_id", entity_id)
        .limit(limit)
        .execute()
    )

    if not relations.data:
        return pd.DataFrame()

    cluster_ids = [r["cluster_id"] for r in relations.data]

    # è·å–èšç±»è¯¦æƒ…
    clusters = (
        _supabase.table("analysis_clusters")
        .select("id, primary_title, summary, category, created_at, article_count")
        .in_("id", cluster_ids)
        .execute()
    )

    if clusters.data:
        return pd.DataFrame(clusters.data)
    return pd.DataFrame()


@st.cache_data(ttl=300)
def get_trending_entities(_supabase, hours: int = 24, limit: int = 10) -> pd.DataFrame:
    """è·å–è¶‹åŠ¿ä¸Šå‡çš„å®ä½“"""
    cutoff = (datetime.now() - timedelta(hours=hours)).isoformat()

    result = (
        _supabase.table("entities")
        .select("*")
        .gte("last_seen", cutoff)
        .eq("trend_direction", "rising")
        .order("mention_count_24h", desc=True)
        .limit(limit)
        .execute()
    )

    if result.data:
        return pd.DataFrame(result.data)
    return pd.DataFrame()


# å®ä½“æ¡£æ¡ˆé¡µ
def render_entities(supabase):
    """æ¸²æŸ“å®ä½“æ¡£æ¡ˆé¡µ"""
    st.markdown('<div class="main-header">ğŸ“ å®ä½“æ¡£æ¡ˆ</div>', unsafe_allow_html=True)

    # è·å–ç­›é€‰æ¡ä»¶
    col1, col2 = st.columns(2)
    with col1:
        entity_type = st.selectbox(
            "å®ä½“ç±»å‹:",
            ["å…¨éƒ¨", "person", "organization", "location", "event", "concept"],
        )
    with col2:
        category = st.selectbox(
            "æ‰€å±åˆ†ç±»:", ["å…¨éƒ¨", "military", "politics", "economy"]
        )

    # è·å–å®ä½“åˆ—è¡¨
    entities_df = get_entities(supabase, entity_type, category, limit=100)

    if entities_df.empty:
        st.info("æš‚æ— å®ä½“æ•°æ®")
        return

    # æ˜¾ç¤ºçƒ­é—¨å®ä½“
    st.markdown("### ğŸ”¥ çƒ­é—¨å®ä½“")

    top_entities = entities_df.head(10)
    cols = st.columns(5)
    for idx, (_, row) in enumerate(top_entities.iterrows()):
        with cols[idx % 5]:
            mention_count = row.get("mention_count_total", 0)
            st.metric(label=row.get("name", "N/A")[:15], value=f"{mention_count}æ¬¡")

    st.markdown("---")

    # å®ä½“åˆ—è¡¨
    st.markdown("### ğŸ“‹ å®ä½“åˆ—è¡¨")

    for idx, row in entities_df.iterrows():
        entity_id = row.get("id")
        entity_name = row.get("name", "N/A")
        entity_type_val = row.get("entity_type", "æœªçŸ¥")
        mention_count = row.get("mention_count_total", 0)
        last_seen = row.get("last_seen", "N/A")

        # è¶‹åŠ¿æŒ‡ç¤º
        trend = row.get("trend_direction", "stable")
        trend_icon = "ğŸ“ˆ" if trend == "rising" else "ğŸ“‰" if trend == "falling" else "â¡ï¸"

        with st.expander(
            f"{trend_icon} {entity_name} ({entity_type_val}) - æåŠ{mention_count}æ¬¡"
        ):
            col1, col2, col3 = st.columns(3)
            with col1:
                st.write(f"**ç±»å‹:** {entity_type_val}")
                st.write(f"**åˆ†ç±»:** {row.get('category', 'N/A')}")
            with col2:
                st.write(f"**24hæåŠ:** {row.get('mention_count_24h', 0)}")
                st.write(f"**7å¤©æåŠ:** {row.get('mention_count_7d', 0)}")
            with col3:
                st.write(f"**è¶‹åŠ¿:** {trend}")
                st.write(f"**æœ€åå‡ºç°:** {str(last_seen)[:10] if last_seen else 'N/A'}")

            # æ˜¾ç¤ºå…³è”èšç±»
            st.markdown("**ç›¸å…³çƒ­ç‚¹:**")
            related_clusters = get_entity_related_clusters(supabase, entity_id, limit=5)
            if not related_clusters.empty:
                for _, cluster in related_clusters.iterrows():
                    st.write(f"- {cluster.get('primary_title', 'N/A')[:60]}...")
            else:
                st.write("æš‚æ— å…³è”çƒ­ç‚¹")

    # å®ä½“ç»Ÿè®¡å›¾è¡¨
    st.markdown("---")
    st.markdown("### ğŸ“Š å®ä½“ç»Ÿè®¡")

    col1, col2 = st.columns(2)
    with col1:
        # æŒ‰ç±»å‹ç»Ÿè®¡
        if not entities_df.empty and "entity_type" in entities_df.columns:
            type_counts = entities_df["entity_type"].value_counts()
            st.bar_chart(type_counts)
    with col2:
        # æŒ‰æåŠæ¬¡æ•°åˆ†å¸ƒ
        if not entities_df.empty and "mention_count_total" in entities_df.columns:
            mention_dist = pd.cut(
                entities_df["mention_count_total"],
                bins=[0, 1, 5, 10, 50, 1000],
                labels=["1æ¬¡", "2-5æ¬¡", "6-10æ¬¡", "11-50æ¬¡", "50+æ¬¡"],
            ).value_counts()
            st.bar_chart(mention_dist)


# æ•°æ®ç»Ÿè®¡é¡µ
def render_stats(supabase):
    """æ¸²æŸ“æ•°æ®ç»Ÿè®¡é¡µ"""
    st.markdown('<div class="main-header">ğŸ“ˆ æ•°æ®ç»Ÿè®¡</div>', unsafe_allow_html=True)

    # è·å–ç»Ÿè®¡æ•°æ®
    stats = get_stats(supabase)

    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("æ€»æ–‡ç« æ•°", f"{stats['articles_total']:,}")
    with col2:
        st.metric(
            "å·²åˆ†ææ–‡ç« ", f"{stats['articles_total'] - stats['articles_unanalyzed']:,}"
        )
    with col3:
        st.metric("å¾…åˆ†ææ–‡ç« ", stats["articles_unanalyzed"])

    st.markdown("---")

    # èšç±»è¶‹åŠ¿
    st.markdown("### ğŸ“Š èšç±»è¶‹åŠ¿ (æœ€è¿‘7å¤©)")

    # è·å–7å¤©æ•°æ®
    days_7 = (datetime.now() - timedelta(days=7)).isoformat()
    clusters_7d = (
        supabase.table("analysis_clusters")
        .select("created_at, category")
        .gte("created_at", days_7)
        .execute()
    )

    if clusters_7d.data:
        df = pd.DataFrame(clusters_7d.data)
        df["created_at"] = pd.to_datetime(df["created_at"]).dt.date

        # æŒ‰å¤©å’Œåˆ†ç±»ç»Ÿè®¡
        daily_counts = (
            df.groupby(["created_at", "category"]).size().unstack(fill_value=0)
        )
        st.line_chart(daily_counts)
    else:
        st.info("æš‚æ— æ•°æ®")

    # åˆ†ç±»å æ¯”
    st.markdown("### ğŸ¥§ åˆ†ç±»å æ¯”")

    all_clusters = supabase.table("analysis_clusters").select("category").execute()
    if all_clusters.data:
        df = pd.DataFrame(all_clusters.data)
        cat_counts = df["category"].value_counts()
        # ä½¿ç”¨æ¡å½¢å›¾ä»£æ›¿é¥¼å›¾
        st.bar_chart(cat_counts)


# ä¸»å‡½æ•°
def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ– Supabase
    supabase = init_supabase()

    if not supabase:
        st.error("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ã€‚è¯·æ£€æŸ¥é…ç½®ã€‚")
        return

    # æ¸²æŸ“ä¾§è¾¹æ 
    page, hours, category = render_sidebar()

    # æ ¹æ®é¡µé¢æ¸²æŸ“å†…å®¹
    if page == "ğŸ  æ¦‚è§ˆé¦–é¡µ":
        render_overview(supabase, hours, category)
    elif page == "ğŸ”¥ çƒ­ç‚¹è¯¦æƒ…":
        render_hotspots(supabase, hours, category)
    elif page == "ğŸ“¡ ä¿¡å·ä¸­å¿ƒ":
        render_signals(supabase, hours)
    elif page == "ğŸ“ å®ä½“æ¡£æ¡ˆ":
        render_entities(supabase)
    elif page == "ğŸ“ˆ æ•°æ®ç»Ÿè®¡":
        render_stats(supabase)


if __name__ == "__main__":
    main()
